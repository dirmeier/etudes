---
title: "Normalizing flows for density estimation"
author: "Simon Dirmeier <simon.dirmeier @ web.de>"
date: "October 2020"
bibliography: ./references/references.bib
link-citations: true
output:
  html_document:
    theme: lumen
    css: ./css/custom.css
    toc: yes
    toc_depth: 1
    toc_float:
      collapsed: no
      smooth_scroll: yes
    number_sections: no
    highlight: pygments
---

```{r knitr_init, include=FALSE, echo=FALSE, cache=FALSE, message=FALSE}
knitr::opts_chunk$set(comment = NA, warning = FALSE, error = FALSE,
                      fig.align = "center",
                      fig.width=11, fig.height=4)

make.moons <- function(n, sigma) {
  seqs <- seq(0, pi, length.out=n)
  
  outer_circ_x = cos(seqs)
  outer_circ_y = sin(seqs)
  inner_circ_x = 1 - cos(seqs)
  inner_circ_y = 1 - sin(seqs) - .5
  X = matrix(
    c(outer_circ_x, inner_circ_x ,
      outer_circ_y, inner_circ_y) +
      rnorm(n * 4 , 0, sigma),
    ncol=2, byrow = F
  )
  
  X
}
```

In this notebook we will implement a normalizing flow, the [masked autoregressive flow](http://papers.nips.cc/paper/6828-masked-autoregressive-flow-for-density-estimation), for density estimation using Tensorflow Probability. A useful review on recent trends on normalizing flows can be found [here](https://arxiv.org/abs/1912.02762). Feedback and comments are welcome!

# Normalizing flows

Normalizing flows (NFs) transform simple *base distributions* into rich, complex families of distributions that can be used for density estimation, variational inference, reparameterization of MCMC, or data generation. NFs express the distribution of a random vector $\mathbf{y} \in \mathbb{R}^p$ by applying a transformation $f$ to some random vector $\mathbf{x}$ sampled from $P_X(\mathbf{x})$:

$$\begin{align}
\mathbf{x} & \sim P_Y(\mathbf{x}) \\
\mathbf{y} &= f(\mathbf{x})
\end{align}$$

The defining property of normalizing flows is that the transformation $f$ is invertible as well as differentiable. In order for these two properties to hold, $\mathbf{x}$ must have the same dimensionality as $\mathbf{y}$. With these conditions, the density of $\mathbf{y}$ is well-defined and reads as:

$$\begin{align}
P_Y(\mathbf{y}) & = P_X\left(f^{-1}(\mathbf{y})\right)\left|\text{det} \frac{\partial f^{-1}}{\partial \mathbf{y}}\right| \\
& = P_X\left(\mathbf{x}\right)  \left| \text{det} \frac{\partial f}{\partial \mathbf{x}}  \right|^{-1}
\end{align}$$

where $\frac{\partial f^{-1}}{\partial \mathbf{y}}$ is the $p \times p$-dimensional Jacobian of $f^{-1}$ w.r.t. $\mathbf{y}$. In practice the transformation $f$ consists of a series of invertible, differentiable functions $f_1, \dots, f_K$:

$$
\mathbf{y} = \mathbf{x}_K = f_K \circ \dots f_2 \circ f_1(\mathbf{x}_0)
$$

The density of this transformation is given by:

$$\begin{align}
P_Y(\mathbf{y}) & = P_X\left(\mathbf{x}_0 \right) \prod_{k=1}^K \left| \text{det} \frac{\partial f_k}{\partial \mathbf{x}_{k - 1}}  \right|^{-1}
\end{align}$$

Computing the determinant of a Jacobian is cubic in $p$. In order to be able to use NFs in production, we would like to be able to efficiently compute the determinants of the Jacobians. Most approaches to NFs achieve that by constructing transformations which have triangular Jacobians for which the determinants can be computed in linear time.

## MAFs

[Masked autoregressive flows](http://papers.nips.cc/paper/6828-masked-autoregressive-flow-for-density-estimation) (MAFs) model the transformation of a sample $\mathbf{x}$ of the base distribution $P_X$ autoregressively as

$$\begin{align}
y_i &= f\left(x_i\right)\\
y_i &= x_i  \exp \left( \alpha_i \right) + \mu_i \\\\
x_i &= f^{-1}\left(y_i\right)\\
x_i &= \left(y_i - \mu_i \right)  \exp \left( -\alpha_i \right)
\end{align}$$

where $\mu_i = f_{\mu_i}\left( \mathbf{y}_{1:i-1}  \right)$ and $\alpha_i = f_{\alpha_i}\left( \mathbf{y}_{1:i-1}  \right)$ are two scalar functions. Due to the autoregressive structure the Jacobian of the inverse function $f^{-1}$ is lower triangular:

$$\begin{align}
\frac{\partial f^{-1}}{\partial \mathbf{y}} = \begin{pmatrix}
\exp(-\alpha_1)&& \mathbf{0}\\ 
&\ddots&\\
\frac{\partial f^{-1}_{2:p}}{\partial \mathbf{y}_{1:p}} && \exp(-\alpha_{2:p})
\end{pmatrix}
\end{align}$$

such that the determinant is merely the product on the diagonal.

In order to make $\mathbf{y}$ have an autoregressive structure, the authors make use of the approach used by [MADE](https://arxiv.org/abs/1502.03509), i.e., an autoencoder that enforces the autoregressive property by multiplying binary masks with the weight matrices of the autoencoder.

# Implementation

In the following we implement the *masked autoregressive flow* and *masked autoregressive distribution estimation* from scratch using TensorFlow Probability. We first load some required libraries.

```{r}
library(ggplot2)
library(ggthemes)

library(tensorflow)
library(tfprobability)
library(keras)

tfk  <- tf$keras
tfkl <- tfk$layers
tfd  <- tfp$distributions
tfb  <- tfp$bijectors
```

To implement a MAF, we first need to be able to compute the underlying autoencoder MADE. MADE uses binary masks to enforce autoregressive structure. To do that we first need to compute degree vectors which describe the maximum number of inputs an element $y_i$ can have.

```{r}
make.degrees <- function(p, hidden.dims) {
  m <- list(tf$constant(seq(p), dtype=tf$float32))
  for (dim in hidden.dims) {
    n.min <- min(min(m[[ length(m) ]]$numpy()), p - 1L)
    degrees <- as.integer(seq(dim) %% max(1L, p - 1L) + min(1L, p  - 1L))
    degrees <- tf$constant(degrees, dtype=tf$float32)
    m <- c(m, degrees)
  }
  
  m
}
```

```{r}
make.degrees(2, c(4, 4))
```

From these we can compute the binary masks. We don't do that exactly as in the original publication for reason explained later.

```{r}
make.masks <- function(degrees) {
  masks <- list()
  len <- length(degrees)
  
  for (i in seq(1, len - 1)) {
    mask <- tf$expand_dims(degrees[[i]], 1L) <= degrees[[i + 1]]
    mask <- tf$cast(mask, tf$float32)
    masks <- c(masks, mask)
  }
  
  masks <- c(
    masks,
    tf$cast(tf$expand_dims(degrees[[len]], 1L) < degrees[[1]], tf$float32)
  )
  
  masks
}
```

```{r}
make.masks(make.degrees(2, c(4, 4)))
```

In order to mask a weight matrix, we can use a Dense layer and apply a constraint (hence this way of encoding the mask and not as in the paper).

```{r}
make.constraint <- function(mask) {
  .f <- function(x) mask * tf$identity(x,  dtype=tf$float32)
  .f
}
```

Likewise we initialize a layer using the same mask such that all elements that are being masked are initialized as zero.

```{r}
make.init <- function(mask) {
  .f <- function(shape, dtype=NULL) {
    mask * tf$keras$initializers$GlorotUniform(23L)(shape)
  }
  .f
}
```

We build the autoencoder as a sequence of Keras layers. Since we are going to need two parameters for every component, i.e., to compute $y_i$ we need to compute $f_{\mu_i}\left(\mathbf{y}_{1:i-1}\right)$ and $f_{\sigma_i}\left(\mathbf{y}_{1:i-1}\right)$, we need the last layer to have $2p$ parameters.

```{r}
make.network <- function(p, hidden.dims, n.params) {
  masks <- make.masks(make.degrees(p, hidden.dims))
  len.masks <- length(masks)
  
  mask <- tf$tile(tf$expand_dims(masks[[len.masks]], 2L), c(1L, 1L, n.params))
  mask <- tf$reshape(mask, shape(mask$shape[[1]], p * n.params))
  masks[[len.masks]] <- mask
  
  network <- tf$keras$models$Sequential(layers=list(
    tf$keras$layers$InputLayer(input_shape=shape(p)))
  )

  dims <- c(hidden.dims, c(p * n.params))
  for (i in seq(masks)) {
    layer <- 
      tf$keras$layers$Dense(
        dims[i],
        kernel_constraint=make.constraint(masks[[i]]),
        kernel_initializer=make.init(masks[[i]]),
        activation=tf$nn$leaky_relu
      )
    network$add(layer)
  }
  network$add(tf$keras$layers$Reshape(shape(p, n.params)))
 
   network
}
```

Let's test this:

```{r}
network <- make.network(2L, c(5L, 5L), 2L)
X <- tfd$Normal(0.0, 1.0)$sample(shape(5, 2))
```

```{r}
network(X)
```

```{r}
network$trainable_variables
```

In order to implement the normalizing flow, we can use TensorFlow Probability's Bijector API. To do that we create a class that inherits form `tfb.Bijector` and override functions for the forward transformation, its inverse and the determinant of the Jacobian. The inverse is easy to compute in a single pass. To sample from $P_Y$ requires performing $p$ sequential passes.

```{r}
maf <- function(shift_and_log_scale_fn) {
  tfb_inline(
    forward_fn = function(x) {
      y <- tf$zeros_like(x, dtype=tf$float32)
      for (i in seq(X$shape[[2]])) {
        sas <- shift_and_log_scale_fn(y)
        sas <- tf$unstack(sas, 2, -1)
        y <- x * tf$math$exp(sas[[2]]) + sas[[1]]
      }
      y
    },
    inverse_fn = function(y) {
      sas <- shift_and_log_scale_fn(y)
      sas <- tf$unstack(sas, 2, -1)
      (y - sas[[1]]) * tf$math$exp(sas[[2]])
    },
    inverse_log_det_jacobian_fn = function(y) {
      sas <- shift_and_log_scale_fn(y)
      sas <- tf$unstack(sas, 2, -1)
      -tf$reduce_sum(sas[[2]], axis=1L)
    },
    forward_min_event_ndims = 1L,
    inverse_min_event_ndims = 1L
  )
}
```

That is all. The inverse and the determinant of its Jacobian are computed as described above. Let's test it:

```{r}
bij <- maf(make.network(2L, c(5L, 5L), 2L))
bij$forward(X)
```

# Density estimation

We will first test our MAF to estimate the density of the moon data set. We can sample from the moon density using `sklearn`.

```{r}
X <- make.moons(5000, 0.05)
X <- tf$constant(X, dtype=tf$float32)
```

```{r}
ggplot(as.data.frame(X$numpy())) + 
  geom_point(aes(V1 ,V2), pch=".") +
  theme_tufte() + 
  theme(
    axis.title=element_blank(),
    axis.text = element_text(size = 12)
  )
```

We implement the NF by stacking several layers of MAF, i.e. multiple functions $f_i$, followed by a permutation of the components.

```{r}
hidden.dims <- c(512L, 512L)
n.layers <- 2L

bijectors <- list()
for (i in seq(n.layers)) {
  bij <- maf(make.network(2L, hidden.dims, 2L))
  bijectors <- c(
    bijectors,
    bij,
    tfb$Permute(shape(1, 0))
  )
}
  
bijectors <- tfb$Chain(
  bijectors=rev(bijectors[1:(length(bijectors) - 1)])
)
```

To sample and be able to compute the log probabilty of a transformation using an NF we wrap the chain of bijectors as `TransformedDistribution` with a standard normal as base distribution.

```{r}
distribution <- tfd$TransformedDistribution(
  distribution=tfd$Normal(loc=0.0, scale=1.0),
  bijector=bijectors
)
```

To fit the density, we can again use TensorFlow's API with the negative log likelihood as loss function

```{r}
x_ <- tfkl$Input(shape=shape(2L), dtype=tf$float32)
log_prob_ <-distribution$log_prob(x_)
model <- tfk$Model(x_, log_prob_)

model$compile(
  optimizer=tf$optimizers$Adam(),
  loss=function(., log_prob) -log_prob
)
```

# License

<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a>

The notebook is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.

# Session info

```{r}
sessionInfo()
```

# References
