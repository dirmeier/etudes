---
title: "Causal inference using tensor-product smoothing splines with structured latent confounders"
author: "Simon Dirmeier"
date: "September 2021"
bibliography: ./references/references.bib
link-citations: true
output:
  html_document:
    theme: lumen
    css: ./css/custom.css
    toc: yes
    toc_depth: 1
    toc_float:
      collapsed: no
      smooth_scroll: yes
    number_sections: no
    highlight: pygments
---


```{r knitr_init, include=FALSE, echo=FALSE, cache=FALSE, message=FALSE}
knitr::opts_chunk$set(comment = NA, warning = FALSE, error = FALSE,
                      fig.align = "center",
                      fig.width=10, fig.height=5)

library(reticulate)
use_condaenv("etudes-dev")
```

While catching up on new literature on causal inference, I discovered a paper for inference of potential outcomes when
observations are confounded in a hierarchical way, i.e., when a latent confounding variable is shared among several observations [@witty2020causal]. The paper uses Gaussian processes (GPs) to elegantly model the functional relationships between data and latent variables and, following @d2019multi, shows that the estimator of the individual treatment effect (ITE) is consistent. Even though consistency of an estimator is a desirable property, for finite data variables of interest are often only weakly identifiable when working with complex nonparametric models (at least in my experience) and the utility of otherwise elegant models for principled statistical data analysis is limited. Unfortunately, the paper neither provides any code to redo the analysis nor shows sampler diagnostics or nor visualizations of posterior distributions.

Hence, in this case study, we will first re-implement the proposed model, examine its MCMC diagnostics, and finally propose a model that is both significantly faster to fit and produces easier posterior geometries to sample from. We implement the models in [Stan](https://github.com/stan-dev/stan).

Feedback and comments are welcome!

We load some libraries for inference and working with data first.

```{python, include=FALSE, echo=FALSE, cache=FALSE, message=FALSE}
import logging
logging.basicConfig(level=logging.ERROR, stream=sys.stdout)

def timer(func):
    from timeit import default_timer    
    def f(*args, **kwargs):
        start = default_timer()
        res = func(*args, **kwargs)
        stop = default_timer()
        print(f"Elapsed time: {stop - start}")
        return res
    return f
```

```{python}
import os
import pandas as pd
import numpy as onp

import jax
from jax import jit
import jax.numpy as np
import jax.scipy as sp
import jax.random as random

from cmdstanpy import CmdStanModel

import matplotlib.pyplot as plt
import seaborn as sns
import arviz as az
import palettes

sns.set_style(
    "ticks", 
    {'font.family':'serif', 
     'font.serif':'Times New Roman'}
)
palettes.set_theme()
```

# Hierarchical confounding

@witty2020causal assume a structural equations model of the following form

$$\begin{align}
u_o & \leftarrow \epsilon_{u_o} \\
x_i & \leftarrow f_X\left(u_{o = \text{Pa}(i)},  \epsilon_{x_i} \right) \\
t_i & \leftarrow f_{T}\left(u_{o = \text{Pa}(i)}, x_i, \epsilon_{t_i} \right) \\
y_i & \leftarrow f_{Y}\left(u_{o = \text{Pa}(i)}, x_i, t_i, \epsilon_{y_i} \right) \\
\end{align}$$

where $o =1, \dots, N_O$ indexes the number of latent confounders $U_o$, $i = 1, \dots, N_I$ indexes covariables
$X_i$, treatments $T_i$ and outcomes $Y_i$ all of which we assume to be univariate for simplicity, but w.l.o.g can also be multivariate.

Before we define the model from @witty2020causal, we generate some data to define the problem we are dealing with. We first define the sample sizes, number of latent confounders, dimensionality of $X$ and $U$ and noise variances:

```{python}
N_O = 20
N_I = N_O * 10
P_X = P_U = 1

var_u = var_x = var_tr = var_y = 0.5
```

```{python}
i_to_o = np.repeat(np.arange(N_O), int(N_I / N_O))
```

We then sample data following a synthetic evaluation from the paper:

```{python}
rng_key = random.PRNGKey(23)
```

```{python}
rng_key, sample_key = random.split(rng_key, 2)
U = random.multivariate_normal(
    sample_key, 
    mean=np.zeros(P_U),
    cov=np.eye(P_U) * var_u,
    shape=(N_O,)
)
```

```{python}
rng_key, sample_key = random.split(rng_key, 2)
X_eps = random.multivariate_normal(
    sample_key, 
    mean=np.zeros(P_X),
    cov=np.eye(P_X) * var_x,
    shape=(N_I,)    
)
X = U[i_to_o] + X_eps
```

```{python}
def gt(x, u, i_to_o):
    xs = np.sum(x * np.sin(x), axis=1)
    us = np.sum(u[i_to_o] * np.sin(u[i_to_o]), axis=1)
    return xs - us
  
rng_key, sample_key = random.split(rng_key, 2)
tr_eps = random.normal(
    sample_key, 
    shape=(N_I,)
) * np.sqrt(var_tr)
tr = gt(X, U, i_to_o) + tr_eps
```

```{python}
def gy(t, x, u, i_to_o):
    ts = t * np.sin(2 * t)
    xs = np.sum(x * np.sin(x), axis=1)
    us = np.sum(u[i_to_o] * np.sin(u[i_to_o]), axis=1)
    return ts + xs + 3 * us
  
rng_key, sample_key = random.split(rng_key, 2)
y_eps = random.normal(
    sample_key, 
    shape=(N_I,)
) * np.sqrt(var_y)
y = gy(tr, X, U, i_to_o) + y_eps
```

Let's visualize this. Note that the functional relationship between $U$ and any other variable $X, T, Y$ is - similar to an ANOVA analysis - basically discrete, since $N_O < N_I$.
```{python}
D = pd.DataFrame({"$U$": U[i_to_o,], "$X$": X[:,0], "$T$": tr, "$Y$": y})

sns.pairplot(
    D,
    palette="black",
    plot_kws=dict(marker="+", color="black"),
    diag_kws=dict(color="black"),
    corner=True
)
plt.show()
```

For Stan, we wrap the data into a dictionary:

```{python}
data = {
    "N_I": N_I,
    "N_O": N_O,
    "i_to_o": onp.asarray(i_to_o + 1),
    "X": onp.asarray(np.squeeze(X)),
    "tr": onp.asarray(tr),
    "y": onp.asarray(y),
    "alpha": 5.0,
    "beta": 5.0
}
```

In addition, we define a sampling method such that we can time inference of posterior distributions of a model.
We sample a total of $3000$ times on two separate chains of which we discard the first $1000$ samples which is usually more than enough for HMC.

```{python}
@timer
def sample(model, data, iter_warmup=1000, iter_sampling=3000):
  return model.sample(
        data=data, 
        iter_warmup=iter_warmup,
        iter_sampling=iter_sampling,
        chains=2, 
        parallel_chains=2, 
        show_progress=False,
        seed=23
    )
```

# Gaussian process model

 @witty2020causal propose a semi-parametric model that models every structural equation using a GP (if I translate this correctly from the paper)

$$\begin{align}
u & \sim  \text{MvNormal}(0, \sigma_U^2 I) \\
x & \sim \text{GP}\left(0, K_X\left(u_{o = \text{Pa}(i)}, u_{o = \text{Pa}(i)}\right) + \sigma_X^2 I \right) \\
t & \sim \text{GP}\left(0, K_T\left(\left[u_{o = \text{Pa}(i)}, x_i \right], \left[u_{o = \text{Pa}(i)}, x_i \right]\right) + \sigma_T^2 I \right) \\
y & \sim \text{GP}\left(0, K_Y\left(\left[u_{o = \text{Pa}(i)}, x_i, t_i \right], \left[u_{o = \text{Pa}(i)}, x_i, t_i \right]\right) + \sigma_Y^2 I \right) \\
\end{align}$$

where the notation $[a, b]$ concatenates the row-vectors $a$ and $b$ along the same axis and every covariance function $K_k$ is an exponentiated-quadratic covariance function with automatic relevance determination, i.e., for every dimension of a feature vector a separate length-scale is introduced. For instance, $K_X\left(u_{o = \text{Pa}(i)}, u_{o = \text{Pa}(i)}\right)$ for univariate $u$ and $i$ uses three hyperparamters.

On first view, this model seems difficult to fit with common probabilistic languages and Hamiltonian Monte Carlo. 
The regression of $X$ on $U$ is a Gaussian process latent variable model which is in itself is not trivial to work with, even though to help identify the kernel parameters of $K_X$ statistical strength can be borrowed from the regressions of $T$ and $Y$ onto $U$. In addition, the posterior geometry looks to be challenging to explore due to the high number of positively-constrained parameters and the somewhat awkward covariance structure of $K_X\left(u_{o = \text{Pa}(i)}, u_{o = \text{Pa}(i)}\right)$. Lastly, for low sample sizes both $u$ and the kernel hyperparameters might be only weakly identified (if at all) which for interpretation of the results is undesirable.

Let's try to fit this model:

```{python}
tps_models_folder = "_models/causal_inference_using_tensor_product_smoothing_splines"
tps_model_file = os.path.join(tps_models_folder, "gp_model.stan")

model = CmdStanModel(stan_file=model_file)
fit = sample(model, data)
```

The fit was tremendously slow which is usually a sign of a very unfavourable posterior geometry. Let's have a look at posterior diagnostics.

```{python}
print(fit.diagnose())
```

Let's also have a look at the energy plot and a trace plot of $U$ and $\sigma_U$

```{python}
posterior_az = az.from_cmdstanpy(fit)

_, ax = plt.subplots(figsize=(8, 4))
az.plot_energy(posterior_az, ax=ax, fill_color=["#233B43", "darkgrey"]);
ax.legend(title="", bbox_to_anchor=(1.2, 0.5))
plt.show()
```

```{python}
_, axes = plt.subplots(ncols=2, nrows=2, figsize=(12, 6))
az.plot_trace(
    posterior_az, axes=axes, var_names=["u_scale", "U"]
    chain_prop={"color": palettes.discrete_qualitative_colors(2)}
);
plt.show()
```

The MCMC diagnostics are worrisome. Not only did the chains not mix, the effective sample size of some parameters is approximately one. We could just sample longer chains, increase the `tree-depth` or decrease `adapt-delta`, but this model seems too pathological to fit successfully (at least for this data set).

I assume the low effective sample size when using HMC was one of the reasons why the authors used an elliptical slice sampler for the confounders. They note however: "*[the model] tends to underestimate the uncertainty in [the counterfactual] estimates. In other words, the posterior density on the ground-truth counterfactual is sometimes low, despite the fact that the mean estimate is close to the ground-truth relative to the baselines. We suspect that this is partially attributable to inaccuracies resulting from our approximate inference procedure*". Hence, it might very well be that their sampling scheme produces the same pathological result (which warrants the question why they didn't include diagnostics given that the model seems either ill-defined or at least hard to work with in practice, and given that there are apparantely *inaccuracies resulting from our approximate inference procedure*).

# A second GP model

The inference of $U$ seems to be problematic. Let's try a simpler model, where we replace the GP regression of $X$ on $U$ with a single linear predictor:

$$\begin{align}
u & \sim  \text{MvNormal}(0, \sigma_U^2 I) \\
\beta & \sim  \text{Normal}(0, 1) \\
x_i & \sim \text{Normal}\left(u_{o = \text{Pa}(i)}\beta, \sigma_X^2 \right) \\
t & \sim \text{GP}\left(0, K_T\left(\left[u_{o = \text{Pa}(i)}, x_i \right], \left[u_{o = \text{Pa}(i)}, x_i \right]\right) + \sigma_T^2 I \right) \\
y & \sim \text{GP}\left(0, K_Y\left(\left[u_{o = \text{Pa}(i)}, x_i, t_i \right], \left[u_{o = \text{Pa}(i)}, x_i, t_i \right]\right) + \sigma_Y^2 I \right) \\
\end{align}$$

This change is somewhat sensible. This $N_O < N_I$ we cannot really estimate a smooth function anyway. Let's fit this


```{python}
tps_model_file = os.path.join(tps_models_folder, "gp+linear_model.stan")

model = CmdStanModel(stan_file=model_file)
fit = sample(model, data)
```

The fit was a bit faster. What are the diagnostics saying?

```{python}
print(fit.diagnose())
```

```{python}
posterior_az = az.from_cmdstanpy(fit)

_, ax = plt.subplots(figsize=(8, 4))
az.plot_energy(posterior_az, ax=ax, fill_color=["#233B43", "darkgrey"]);
ax.legend(title="", bbox_to_anchor=(1.2, 0.5))
plt.show()
```

The chains still don't seem to converge! The effective sample sizes are worrisome, too! Apparantly the GP hyperparameter still do not allow efficiently exploring the posterior manifold.

# A tensor-product smoothing spline model



# License

<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a>

The notebook is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.

# Session info

```{python, echo=FALSE}
import session_info
session_info.show(html=False)
```

# Stan files

```{python, echo=FALSE}
print(open(tps_model_file, "r").read())
```

```{python, echo=FALSE}
print(open(tps_model_gq_file, "r").read())
```


# References
