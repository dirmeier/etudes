{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "    library(rstan)\n",
    "    library(bayesplot)\n",
    "    library(tidyverse)\n",
    "    library(cowplot)  \n",
    "})\n",
    "options(repr.plot.width = 6, repr.plot.height = 3)\n",
    "theme_set(theme_cowplot()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- 100\n",
    "p <- 100\n",
    "prob <- 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X <- matrix(rnorm(n * p), n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "b <- sapply(seq(p), function(.) {\n",
    "    if (rbinom(1, 1, prob)) {\n",
    "        if (rbinom(1, 1, .5)) {\n",
    "            rnorm(1, 10)\n",
    "        } else rnorm(1, -10)\n",
    "    } else rnorm(1, 0.2)         \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAFoCAIAAADxRFtOAAAABmJLR0QA/wD/AP+gvaeTAAAg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\n/wKLvBhS4yKfuQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.frame(idx=seq(b), coeffs=b) %>%\n",
    "    ggplot(aes(idx, coeffs)) +\n",
    "    geom_line() +\n",
    "    hline_0(color=\"black\", lty=\"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y <- X %*%b + rnorm(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data {\n",
      "  int<lower=1> N; // Number of data\n",
      "  int<lower=1> M; // Number of covariates\n",
      "  matrix[N, M] X;\n",
      "  real y[N];\n",
      "}\n",
      "\n",
      "// slab_scale = 5, slab_df = 25 -> 8 divergences\n",
      "\n",
      "transformed data {\n",
      "  real m0 = 10;           // Expected number of large slopes\n",
      "  real slab_scale = 3;    // Scale for large slopes\n",
      "  real slab_scale2 = square(slab_scale);\n",
      "  real slab_df = 25;      // Effective degrees of freedom for large slopes\n",
      "  real half_slab_df = 0.5 * slab_df;\n",
      "}\n",
      "\n",
      "parameters {\n",
      "  vector[M] beta_tilde;\n",
      "  vector<lower=0>[M] lambda;\n",
      "  real<lower=0> c2_tilde;\n",
      "  real<lower=0> tau_tilde;\n",
      "  real alpha;\n",
      "  real<lower=0> sigma;\n",
      "}\n",
      "\n",
      "transformed parameters {\n",
      "  vector[M] beta;\n",
      "  {\n",
      "    real tau0 = (m0 / (M - m0)) * (sigma / sqrt(1.0 * N));\n",
      "    real tau = tau0 * tau_tilde; // tau ~ cauchy(0, tau0)\n",
      "\n",
      "    // c2 ~ inv_gamma(half_slab_df, half_slab_df * slab_scale2)\n",
      "    // Implies that marginally beta ~ student_t(slab_df, 0, slab_scale)\n",
      "    real c2 = slab_scale2 * c2_tilde;\n",
      "\n",
      "    vector[M] lambda_tilde =\n",
      "      sqrt( c2 * square(lambda) ./ (c2 + square(tau) * square(lambda)) );\n",
      "\n",
      "    // beta ~ normal(0, tau * lambda_tilde)\n",
      "    beta = tau * lambda_tilde .* beta_tilde;\n",
      "  }\n",
      "}\n",
      "\n",
      "model {\n",
      "  beta_tilde ~ normal(0, 1);\n",
      "  lambda ~ cauchy(0, 1);\n",
      "  tau_tilde ~ cauchy(0, 1);\n",
      "  c2_tilde ~ inv_gamma(half_slab_df, half_slab_df);\n",
      "\n",
      "  alpha ~ normal(0, 2);\n",
      "  sigma ~ normal(0, 2);\n",
      "\n",
      "  y ~ normal(X * beta + alpha, sigma);\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model.file <- \"_models/finnish_horse.stan\"\n",
    "cat(readLines(model.file), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- list(\n",
    "    N=n,\n",
    "    M=p,\n",
    "    X=X,    \n",
    "    y=as.numeric(y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAMPLING FOR MODEL 'finnish_horse' NOW (CHAIN 1).\n",
      "Chain 1: \n",
      "Chain 1: Gradient evaluation took 0.00012 seconds\n",
      "Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.2 seconds.\n",
      "Chain 1: Adjust your expectations accordingly!\n",
      "Chain 1: \n",
      "Chain 1: \n",
      "Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "Chain 1: \n",
      "Chain 1:  Elapsed Time: 13.1278 seconds (Warm-up)\n",
      "Chain 1:                9.31339 seconds (Sampling)\n",
      "Chain 1:                22.4412 seconds (Total)\n",
      "Chain 1: \n",
      "\n",
      "SAMPLING FOR MODEL 'finnish_horse' NOW (CHAIN 2).\n",
      "Chain 2: \n",
      "Chain 2: Gradient evaluation took 6.5e-05 seconds\n",
      "Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.65 seconds.\n",
      "Chain 2: Adjust your expectations accordingly!\n",
      "Chain 2: \n",
      "Chain 2: \n",
      "Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "Chain 2: \n",
      "Chain 2:  Elapsed Time: 13.0281 seconds (Warm-up)\n",
      "Chain 2:                12.8409 seconds (Sampling)\n",
      "Chain 2:                25.869 seconds (Total)\n",
      "Chain 2: \n",
      "\n",
      "SAMPLING FOR MODEL 'finnish_horse' NOW (CHAIN 3).\n",
      "Chain 3: \n",
      "Chain 3: Gradient evaluation took 9.5e-05 seconds\n",
      "Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.95 seconds.\n",
      "Chain 3: Adjust your expectations accordingly!\n",
      "Chain 3: \n",
      "Chain 3: \n",
      "Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "Chain 3: \n",
      "Chain 3:  Elapsed Time: 13.6156 seconds (Warm-up)\n",
      "Chain 3:                10.5273 seconds (Sampling)\n",
      "Chain 3:                24.1429 seconds (Total)\n",
      "Chain 3: \n",
      "\n",
      "SAMPLING FOR MODEL 'finnish_horse' NOW (CHAIN 4).\n",
      "Chain 4: \n",
      "Chain 4: Gradient evaluation took 9.2e-05 seconds\n",
      "Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.92 seconds.\n",
      "Chain 4: Adjust your expectations accordingly!\n",
      "Chain 4: \n",
      "Chain 4: \n",
      "Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "Chain 4: \n",
      "Chain 4:  Elapsed Time: 12.5494 seconds (Warm-up)\n",
      "Chain 4:                14.6933 seconds (Sampling)\n",
      "Chain 4:                27.2427 seconds (Total)\n",
      "Chain 4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“There were 7 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See\n",
      "http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup”Warning message:\n",
      "“Examine the pairs() plot to diagnose sampling problems\n",
      "”Warning message:\n",
      "“Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\n",
      "Running the chains for more iterations may help. See\n",
      "http://mc-stan.org/misc/warnings.html#bulk-ess”"
     ]
    }
   ],
   "source": [
    "finnish_fit <- stan(file=model.file, data=data, seed=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inference for Stan model: finnish_horse.\n",
       "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
       "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
       "\n",
       "                   mean se_mean      sd    2.5%     25%     50%     75%   97.5%\n",
       "beta_tilde[1]     -0.13    0.01    0.52   -1.38   -0.31   -0.07    0.07    1.00\n",
       "beta_tilde[2]     -0.21    0.01    0.53   -1.55   -0.39   -0.12    0.01    0.85\n",
       "beta_tilde[3]      2.30    0.01    0.37    1.69    2.05    2.26    2.49    3.17\n",
       "beta_tilde[4]     -1.13    0.01    0.49   -2.35   -1.39   -0.99   -0.73   -0.54\n",
       "beta_tilde[5]      0.96    0.01    0.53    0.34    0.53    0.83    1.28    2.21\n",
       "beta_tilde[6]     -0.49    0.01    0.46   -1.74   -0.67   -0.35   -0.16   -0.02\n",
       "beta_tilde[7]      0.28    0.01    0.52   -0.70    0.04    0.18    0.47    1.54\n",
       "beta_tilde[8]      0.52    0.01    0.47    0.04    0.18    0.39    0.71    1.78\n",
       "beta_tilde[9]     -0.86    0.01    0.50   -2.13   -1.13   -0.74   -0.47   -0.26\n",
       "beta_tilde[10]    -0.01    0.01    0.54   -1.20   -0.20   -0.01    0.18    1.20\n",
       "beta_tilde[11]    -0.86    0.01    0.51   -2.17   -1.14   -0.75   -0.46   -0.26\n",
       "beta_tilde[12]     0.74    0.01    0.49    0.17    0.35    0.61    0.99    2.01\n",
       "beta_tilde[13]    -0.11    0.01    0.54   -1.36   -0.29   -0.05    0.08    1.06\n",
       "beta_tilde[14]    -0.45    0.01    0.48   -1.69   -0.65   -0.32   -0.13    0.12\n",
       "beta_tilde[15]     0.55    0.01    0.46    0.05    0.20    0.42    0.77    1.75\n",
       "beta_tilde[16]    -0.86    0.01    0.50   -2.13   -1.12   -0.74   -0.47   -0.27\n",
       "beta_tilde[17]     0.35    0.01    0.48   -0.41    0.08    0.24    0.55    1.56\n",
       "beta_tilde[18]     0.94    0.01    0.49    0.36    0.55    0.82    1.22    2.14\n",
       "beta_tilde[19]     0.91    0.01    0.51    0.32    0.51    0.80    1.19    2.14\n",
       "beta_tilde[20]    -0.65    0.01    0.48   -1.87   -0.89   -0.52   -0.29   -0.11\n",
       "beta_tilde[21]     0.89    0.01    0.50    0.29    0.49    0.77    1.17    2.08\n",
       "beta_tilde[22]     0.79    0.01    0.50    0.21    0.41    0.67    1.06    2.03\n",
       "beta_tilde[23]    -0.82    0.01    0.52   -2.11   -1.09   -0.69   -0.41   -0.22\n",
       "beta_tilde[24]     1.22    0.01    0.50    0.64    0.84    1.08    1.49    2.44\n",
       "beta_tilde[25]    -0.89    0.01    0.53   -2.21   -1.16   -0.76   -0.48   -0.28\n",
       "beta_tilde[26]     1.01    0.01    0.50    0.43    0.62    0.89    1.28    2.25\n",
       "beta_tilde[27]     0.38    0.01    0.46   -0.19    0.10    0.25    0.57    1.62\n",
       "beta_tilde[28]     0.89    0.01    0.50    0.29    0.49    0.77    1.17    2.17\n",
       "beta_tilde[29]     0.68    0.01    0.48    0.14    0.32    0.56    0.92    1.91\n",
       "beta_tilde[30]    -0.59    0.01    0.48   -1.83   -0.81   -0.46   -0.23   -0.08\n",
       "beta_tilde[31]     0.00    0.01    0.55   -1.28   -0.19    0.00    0.19    1.20\n",
       "beta_tilde[32]    -0.80    0.01    0.51   -2.06   -1.06   -0.67   -0.40   -0.20\n",
       "beta_tilde[33]    -1.03    0.01    0.50   -2.24   -1.31   -0.92   -0.63   -0.44\n",
       "beta_tilde[34]    -0.62    0.01    0.47   -1.87   -0.84   -0.50   -0.27   -0.10\n",
       "beta_tilde[35]    -0.96    0.01    0.50   -2.19   -1.23   -0.84   -0.56   -0.36\n",
       "beta_tilde[36]    -0.50    0.01    0.49   -1.73   -0.72   -0.36   -0.15   -0.02\n",
       "beta_tilde[37]    -0.90    0.01    0.50   -2.10   -1.18   -0.79   -0.49   -0.31\n",
       "beta_tilde[38]     3.05    0.01    0.40    2.31    2.77    3.02    3.30    3.90\n",
       "beta_tilde[39]    -0.63    0.01    0.49   -1.88   -0.86   -0.50   -0.26   -0.09\n",
       "beta_tilde[40]     0.91    0.01    0.52    0.31    0.50    0.78    1.18    2.23\n",
       "beta_tilde[41]     0.66    0.01    0.46    0.13    0.30    0.54    0.88    1.80\n",
       "beta_tilde[42]     0.66    0.01    0.49    0.11    0.29    0.54    0.91    1.93\n",
       "beta_tilde[43]     0.91    0.01    0.50    0.32    0.51    0.80    1.19    2.15\n",
       "beta_tilde[44]     0.81    0.01    0.50    0.23    0.43    0.69    1.08    2.06\n",
       "beta_tilde[45]     0.63    0.01    0.47    0.11    0.27    0.50    0.87    1.84\n",
       "beta_tilde[46]    -0.98    0.01    0.51   -2.22   -1.25   -0.85   -0.57   -0.39\n",
       "beta_tilde[47]    -0.71    0.01    0.48   -1.93   -0.96   -0.60   -0.33   -0.15\n",
       "beta_tilde[48]     0.65    0.01    0.48    0.11    0.28    0.52    0.88    1.86\n",
       "beta_tilde[49]     0.34    0.01    0.53   -0.61    0.07    0.23    0.55    1.69\n",
       "beta_tilde[50]    -0.73    0.01    0.50   -1.97   -0.99   -0.60   -0.34   -0.16\n",
       "beta_tilde[51]    -0.68    0.01    0.50   -2.02   -0.93   -0.54   -0.30   -0.12\n",
       "beta_tilde[52]    -0.69    0.01    0.49   -1.97   -0.94   -0.56   -0.31   -0.13\n",
       "beta_tilde[53]    -0.72    0.01    0.52   -2.07   -0.97   -0.59   -0.33   -0.15\n",
       "beta_tilde[54]    -0.41    0.01    0.48   -1.69   -0.60   -0.29   -0.11    0.23\n",
       "beta_tilde[55]     0.94    0.01    0.51    0.34    0.53    0.82    1.22    2.23\n",
       "beta_tilde[56]    -0.90    0.01    0.50   -2.12   -1.17   -0.77   -0.50   -0.31\n",
       "beta_tilde[57]    -0.58    0.01    0.47   -1.80   -0.79   -0.45   -0.23   -0.08\n",
       "beta_tilde[58]     1.17    0.01    0.50    0.57    0.78    1.03    1.44    2.44\n",
       "beta_tilde[59]    -0.81    0.01    0.50   -2.04   -1.08   -0.69   -0.43   -0.23\n",
       "beta_tilde[60]    -1.02    0.01    0.50   -2.25   -1.30   -0.89   -0.61   -0.43\n",
       "beta_tilde[61]     0.80    0.01    0.49    0.22    0.42    0.69    1.06    2.01\n",
       "beta_tilde[62]     0.20    0.01    0.51   -0.84    0.00    0.12    0.41    1.36\n",
       "beta_tilde[63]    -0.76    0.01    0.50   -2.03   -1.02   -0.65   -0.37   -0.18\n",
       "beta_tilde[64]     0.84    0.01    0.53    0.22    0.43    0.71    1.11    2.14\n",
       "beta_tilde[65]    -0.88    0.01    0.50   -2.08   -1.17   -0.76   -0.48   -0.28\n",
       "beta_tilde[66]    -0.67    0.01    0.48   -1.91   -0.92   -0.54   -0.29   -0.12\n",
       "beta_tilde[67]     2.42    0.01    0.37    1.80    2.16    2.39    2.63    3.26\n",
       "beta_tilde[68]     0.80    0.01    0.51    0.21    0.40    0.67    1.07    2.08\n",
       "beta_tilde[69]     0.87    0.01    0.51    0.26    0.46    0.75    1.15    2.10\n",
       "beta_tilde[70]    -0.44    0.01    0.47   -1.66   -0.64   -0.30   -0.13    0.07\n",
       "beta_tilde[71]    -0.35    0.01    0.49   -1.57   -0.55   -0.23   -0.07    0.39\n",
       "beta_tilde[72]    -0.66    0.01    0.47   -1.88   -0.91   -0.53   -0.30   -0.13\n",
       "beta_tilde[73]     0.60    0.01    0.48    0.09    0.24    0.46    0.82    1.89\n",
       "beta_tilde[74]     0.89    0.01    0.51    0.29    0.48    0.77    1.17    2.15\n",
       "beta_tilde[75]     0.35    0.01    0.49   -0.38    0.08    0.23    0.54    1.60\n",
       "beta_tilde[76]     1.11    0.01    0.50    0.51    0.71    0.97    1.38    2.33\n",
       "beta_tilde[77]    -0.94    0.01    0.51   -2.18   -1.23   -0.82   -0.53   -0.34\n",
       "beta_tilde[78]     0.74    0.01    0.49    0.16    0.35    0.63    1.00    1.96\n",
       "beta_tilde[79]    -0.99    0.01    0.51   -2.27   -1.24   -0.86   -0.58   -0.40\n",
       "beta_tilde[80]     1.02    0.01    0.51    0.43    0.61    0.88    1.30    2.28\n",
       "beta_tilde[81]     0.99    0.01    0.51    0.40    0.59    0.86    1.26    2.26\n",
       "beta_tilde[82]    -3.06    0.01    0.40   -3.88   -3.31   -3.05   -2.78   -2.33\n",
       "beta_tilde[83]    -0.90    0.01    0.52   -2.17   -1.19   -0.78   -0.48   -0.29\n",
       "beta_tilde[84]     0.77    0.01    0.50    0.19    0.38    0.65    1.04    2.00\n",
       "beta_tilde[85]    -0.16    0.01    0.54   -1.47   -0.35   -0.09    0.03    1.00\n",
       "beta_tilde[86]     0.70    0.01    0.48    0.15    0.33    0.59    0.95    1.88\n",
       "beta_tilde[87]     0.77    0.01    0.50    0.19    0.37    0.64    1.02    2.03\n",
       "beta_tilde[88]    -0.83    0.01    0.51   -2.09   -1.11   -0.71   -0.42   -0.23\n",
       "beta_tilde[89]    -0.74    0.01    0.50   -2.02   -1.00   -0.60   -0.35   -0.17\n",
       "beta_tilde[90]     0.72    0.01    0.48    0.16    0.34    0.61    0.99    1.88\n",
       "beta_tilde[91]    -0.77    0.01    0.51   -2.08   -1.04   -0.64   -0.37   -0.18\n",
       "beta_tilde[92]     0.88    0.01    0.49    0.29    0.49    0.77    1.18    2.08\n",
       "beta_tilde[93]    -0.90    0.01    0.49   -2.17   -1.17   -0.79   -0.51   -0.31\n",
       "beta_tilde[94]    -0.59    0.01    0.46   -1.79   -0.80   -0.46   -0.24   -0.08\n",
       "beta_tilde[95]    -0.80    0.01    0.49   -2.02   -1.06   -0.69   -0.42   -0.21\n",
       "beta_tilde[96]    -0.76    0.01    0.50   -1.98   -1.02   -0.65   -0.36   -0.17\n",
       "beta_tilde[97]     1.18    0.01    0.47    0.62    0.82    1.05    1.44    2.35\n",
       "beta_tilde[98]    -0.98    0.01    0.51   -2.24   -1.26   -0.87   -0.57   -0.38\n",
       "beta_tilde[99]    -0.50    0.01    0.48   -1.74   -0.69   -0.36   -0.17   -0.02\n",
       "beta_tilde[100]    0.69    0.01    0.51    0.13    0.30    0.54    0.94    1.96\n",
       "lambda[1]          1.04    0.13    8.05    0.01    0.08    0.21    0.58    4.28\n",
       "lambda[2]          0.98    0.15    9.38    0.01    0.09    0.23    0.56    4.44\n",
       "lambda[3]         41.04    7.47  340.59    2.44    5.09    8.78   18.25  203.65\n",
       "lambda[4]         12.69    2.71  151.50    0.75    1.40    2.28    4.62   51.76\n",
       "lambda[5]         15.86   10.20  610.18    0.47    0.91    1.50    2.91   31.74\n",
       "lambda[6]          1.34    0.14    7.62    0.04    0.20    0.40    0.86    5.60\n",
       "lambda[7]          1.22    0.22   13.94    0.01    0.11    0.29    0.67    4.70\n",
       "lambda[8]          2.71    0.93   49.81    0.06    0.23    0.45    0.92    5.95\n",
       "lambda[9]         17.13   11.82  733.07    0.35    0.72    1.16    2.09   15.96\n",
       "lambda[10]         0.99    0.14    7.17    0.01    0.08    0.21    0.58    4.73\n",
       "lambda[11]         3.94    0.40   21.72    0.36    0.74    1.18    2.27   23.05\n",
       "lambda[12]         3.48    1.30   59.23    0.24    0.52    0.88    1.65   12.31\n",
       "lambda[13]         1.47    0.38   24.14    0.01    0.08    0.21    0.55    4.70\n",
       "lambda[14]         2.01    0.74   46.12    0.03    0.18    0.39    0.86    5.78\n",
       "lambda[15]         3.12    1.08   50.04    0.08    0.26    0.50    1.02    7.91\n",
       "lambda[16]         4.76    0.77   44.01    0.39    0.75    1.20    2.19   18.15\n",
       "lambda[17]         1.12    0.19   12.21    0.02    0.14    0.32    0.72    5.29\n",
       "lambda[18]         6.07    0.75   42.93    0.49    0.95    1.53    2.86   24.95\n",
       "lambda[19]        23.74   12.48  643.73    0.44    0.87    1.38    2.65   29.24\n",
       "lambda[20]         3.47    1.03   57.62    0.15    0.39    0.68    1.28   10.45\n",
       "lambda[21]         5.35    1.78  111.47    0.42    0.79    1.26    2.39   16.45\n",
       "lambda[22]         3.17    0.29   15.26    0.30    0.63    1.02    1.89   17.62\n",
       "lambda[23]        40.29   33.88 1991.94    0.31    0.62    1.05    1.99   23.84\n",
       "lambda[24]        11.94    1.57   79.03    0.85    1.65    2.75    5.81   65.01\n",
       "lambda[25]        11.53    4.37  230.81    0.38    0.79    1.27    2.39   28.61\n",
       "lambda[26]         8.00    1.45   84.62    0.59    1.14    1.82    3.48   30.23\n",
       "lambda[27]         1.57    0.45   28.05    0.02    0.16    0.35    0.79    5.17\n",
       "lambda[28]         5.54    1.70   78.69    0.39    0.79    1.25    2.27   18.16\n",
       "lambda[29]         2.14    0.19    9.67    0.19    0.46    0.78    1.49   11.50\n",
       "lambda[30]         2.83    0.73   38.56    0.12    0.32    0.59    1.21    9.64\n",
       "lambda[31]         0.78    0.07    4.14    0.01    0.07    0.21    0.55    3.67\n",
       "lambda[32]         4.77    1.58   97.70    0.30    0.63    1.02    1.98   16.19\n",
       "lambda[33]         6.74    1.36   85.25    0.60    1.14    1.81    3.42   29.70\n",
       "lambda[34]         3.62    1.50   92.01    0.15    0.37    0.64    1.22   10.34\n",
       "lambda[35]        15.87    9.63  432.95    0.50    1.01    1.59    2.99   31.30\n",
       "lambda[36]         1.79    0.35   16.54    0.04    0.20    0.41    0.89    8.35\n",
       "lambda[37]        14.66    9.40  420.94    0.43    0.82    1.32    2.52   19.69\n",
       "lambda[38]        50.91    7.33  405.05    3.41    7.16   12.10   25.84  265.90\n",
       "lambda[39]        15.74   12.57  774.91    0.14    0.35    0.62    1.26   11.17\n",
       "lambda[40]        18.50    8.86  555.62    0.43    0.87    1.41    2.60   26.44\n",
       "lambda[41]         4.97    1.95  117.33    0.19    0.43    0.73    1.41   10.31\n",
       "lambda[42]         2.47    0.32   16.40    0.16    0.40    0.69    1.35   11.42\n",
       "lambda[43]         4.46    0.47   23.15    0.46    0.90    1.45    2.74   23.04\n",
       "lambda[44]         5.44    1.50   91.78    0.32    0.66    1.06    1.93   16.20\n",
       "lambda[45]         2.68    0.97   50.98    0.15    0.37    0.66    1.24    8.64\n",
       "lambda[46]         6.20    0.71   39.21    0.54    1.05    1.67    3.21   29.98\n",
       "lambda[47]        79.39   72.63 4283.86    0.23    0.48    0.82    1.61   11.04\n",
       "lambda[48]       191.42  187.91 8294.77    0.16    0.38    0.66    1.29   12.44\n",
       "lambda[49]         1.17    0.15    8.62    0.01    0.13    0.32    0.78    5.46\n",
       "lambda[50]         3.45    0.75   35.92    0.23    0.51    0.86    1.70   13.73\n",
       "lambda[51]         7.38    4.62  290.24    0.19    0.44    0.76    1.54   14.07\n",
       "lambda[52]         2.59    0.31   15.20    0.19    0.44    0.74    1.48   13.70\n",
       "lambda[53]         4.17    1.08   50.06    0.22    0.48    0.82    1.58   13.51\n",
       "lambda[54]         1.48    0.23   13.41    0.02    0.16    0.34    0.78    5.14\n",
       "lambda[55]         7.76    1.38   68.49    0.46    0.94    1.46    2.88   27.33\n",
       "lambda[56]         7.76    2.64  135.76    0.41    0.84    1.37    2.47   26.19\n",
       "lambda[57]         3.64    0.91   45.02    0.11    0.32    0.58    1.15   10.23\n",
       "lambda[58]        57.01   34.58 1428.13    0.80    1.53    2.45    4.92   54.07\n",
       "lambda[59]         3.63    0.41   22.57    0.32    0.66    1.08    2.02   17.98\n",
       "lambda[60]         7.22    0.91   47.19    0.60    1.13    1.82    3.59   31.51\n",
       "lambda[61]         4.03    0.61   28.87    0.31    0.64    1.01    1.87   15.83\n",
       "lambda[62]         1.18    0.22   10.60    0.01    0.09    0.26    0.64    4.85\n",
       "lambda[63]        13.26    9.80  428.11    0.27    0.57    0.94    1.77   14.24\n",
       "lambda[64]         8.11    3.53  194.74    0.31    0.65    1.06    1.97   23.16\n",
       "lambda[65]         4.28    1.24   56.76    0.38    0.76    1.23    2.27   16.90\n",
       "lambda[66]         2.08    0.25   13.17    0.17    0.40    0.71    1.40   10.23\n",
       "lambda[67]        59.29   19.83  957.28    2.61    5.33    9.36   18.87  163.92\n",
       "lambda[68]         3.51    0.32   17.92    0.29    0.61    1.00    1.96   20.65\n",
       "lambda[69]         7.91    2.49   81.99    0.38    0.74    1.20    2.28   25.53\n",
       "lambda[70]         1.17    0.11    6.44    0.03    0.18    0.38    0.84    5.51\n",
       "lambda[71]         1.07    0.08    4.53    0.02    0.14    0.33    0.74    5.80\n",
       "lambda[72]         2.43    0.28   13.48    0.18    0.42    0.72    1.41   10.89\n",
       "lambda[73]         2.12    0.44   27.48    0.12    0.31    0.57    1.11    8.12\n",
       "lambda[74]         5.12    0.78   34.71    0.39    0.80    1.27    2.48   25.82\n",
       "lambda[75]         1.18    0.20   12.08    0.01    0.13    0.32    0.74    4.54\n",
       "lambda[76]        17.09    8.36  398.57    0.70    1.36    2.21    4.36   42.44\n",
       "lambda[77]        62.97   49.69 2760.23    0.45    0.90    1.41    2.68   30.78\n",
       "lambda[78]         2.51    0.27   15.86    0.24    0.51    0.86    1.66   12.41\n",
       "lambda[79]         6.04    0.76   40.15    0.53    1.06    1.70    3.26   28.50\n",
       "lambda[80]         7.55    1.31   63.04    0.57    1.11    1.80    3.45   33.83\n",
       "lambda[81]         8.77    2.35  145.96    0.55    1.06    1.69    3.20   27.44\n",
       "lambda[82]        78.59   22.71 1418.83    3.47    7.20   12.22   25.81  318.19\n",
       "lambda[83]         5.84    1.16   57.01    0.40    0.78    1.25    2.44   22.85\n",
       "lambda[84]         3.33    0.43   23.36    0.28    0.58    0.96    1.88   17.15\n",
       "lambda[85]         1.03    0.14    7.74    0.01    0.08    0.22    0.59    4.39\n",
       "lambda[86]         2.53    0.29   16.76    0.23    0.48    0.82    1.59   11.69\n",
       "lambda[87]         3.23    0.27   14.70    0.26    0.58    0.97    1.84   18.49\n",
       "lambda[88]         4.68    1.11   53.94    0.32    0.66    1.08    2.02   16.35\n",
       "lambda[89]        14.70   11.48  723.98    0.23    0.52    0.88    1.63   14.24\n",
       "lambda[90]         5.72    3.59  199.62    0.22    0.48    0.79    1.53   12.11\n",
       "lambda[91]         2.85    0.33   19.30    0.25    0.54    0.89    1.72   14.13\n",
       "lambda[92]         4.81    0.83   48.80    0.40    0.79    1.26    2.33   16.69\n",
       "lambda[93]         4.43    0.64   35.97    0.41    0.84    1.32    2.37   18.92\n",
       "lambda[94]         2.55    0.69   27.03    0.13    0.33    0.59    1.15    8.15\n",
       "lambda[95]         3.92    0.70   36.88    0.31    0.63    1.00    1.86   16.91\n",
       "lambda[96]         2.60    0.24   12.27    0.26    0.53    0.87    1.69   11.64\n",
       "lambda[97]         7.46    0.63   34.31    0.87    1.63    2.63    5.24   40.75\n",
       "lambda[98]         4.98    0.62   32.68    0.51    1.00    1.58    3.04   23.99\n",
       "lambda[99]         1.31    0.10    5.43    0.06    0.23    0.45    0.92    7.28\n",
       "lambda[100]        2.79    0.49   30.52    0.18    0.40    0.72    1.38   11.16\n",
       "c2_tilde           1.70    0.01    0.46    1.06    1.39    1.63    1.94    2.77\n",
       "tau_tilde        171.49    0.75   37.95  112.77  144.63  166.81  192.52  257.77\n",
       "alpha             -0.08    0.00    0.10   -0.26   -0.14   -0.08   -0.01    0.11\n",
       "sigma              0.98    0.00    0.07    0.86    0.93    0.98    1.03    1.13\n",
       "beta[1]           -0.03    0.00    0.09   -0.22   -0.08   -0.02    0.02    0.13\n",
       "beta[2]           -0.05    0.00    0.09   -0.23   -0.11   -0.05    0.00    0.10\n",
       "beta[3]            7.90    0.00    0.10    7.70    7.83    7.90    7.96    8.09\n",
       "beta[4]           -2.30    0.00    0.09   -2.46   -2.36   -2.29   -2.24   -2.13\n",
       "beta[5]            1.42    0.00    0.12    1.18    1.34    1.42    1.50    1.65\n",
       "beta[6]           -0.19    0.00    0.09   -0.35   -0.25   -0.19   -0.13   -0.01\n",
       "beta[7]            0.09    0.00    0.10   -0.10    0.02    0.08    0.16    0.30\n",
       "beta[8]            0.23    0.00    0.10    0.03    0.17    0.23    0.30    0.42\n",
       "beta[9]           -1.02    0.00    0.09   -1.20   -1.08   -1.02   -0.96   -0.83\n",
       "beta[10]           0.00    0.00    0.09   -0.19   -0.06    0.00    0.05    0.18\n",
       "beta[11]          -1.05    0.00    0.10   -1.24   -1.12   -1.05   -0.99   -0.86\n",
       "beta[12]           0.66    0.00    0.11    0.45    0.59    0.66    0.73    0.87\n",
       "beta[13]          -0.02    0.00    0.08   -0.20   -0.08   -0.02    0.02    0.14\n",
       "beta[14]          -0.17    0.00    0.11   -0.38   -0.25   -0.17   -0.10    0.02\n",
       "beta[15]           0.28    0.00    0.11    0.06    0.20    0.28    0.35    0.49\n",
       "beta[16]          -1.06    0.00    0.09   -1.25   -1.13   -1.06   -1.00   -0.88\n",
       "beta[17]           0.12    0.00    0.10   -0.06    0.05    0.12    0.20    0.33\n",
       "beta[18]           1.43    0.00    0.09    1.25    1.37    1.43    1.49    1.61\n",
       "beta[19]           1.28    0.00    0.09    1.11    1.22    1.28    1.34    1.46\n",
       "beta[20]          -0.44    0.00    0.11   -0.66   -0.52   -0.44   -0.37   -0.21\n",
       "beta[21]           1.14    0.00    0.10    0.95    1.08    1.15    1.21    1.33\n",
       "beta[22]           0.83    0.00    0.09    0.65    0.77    0.83    0.89    1.01\n",
       "beta[23]          -0.87    0.00    0.09   -1.06   -0.93   -0.87   -0.81   -0.69\n",
       "beta[24]           2.76    0.00    0.09    2.58    2.70    2.76    2.83    2.95\n",
       "beta[25]          -1.14    0.00    0.10   -1.34   -1.21   -1.14   -1.08   -0.94\n",
       "beta[26]           1.76    0.00    0.09    1.57    1.70    1.76    1.82    1.94\n",
       "beta[27]           0.13    0.00    0.09   -0.04    0.07    0.13    0.20    0.32\n",
       "beta[28]           1.13    0.00    0.10    0.94    1.07    1.13    1.20    1.33\n",
       "beta[29]           0.54    0.00    0.11    0.33    0.47    0.54    0.62    0.76\n",
       "beta[30]          -0.35    0.00    0.11   -0.56   -0.42   -0.34   -0.28   -0.14\n",
       "beta[31]           0.00    0.00    0.08   -0.16   -0.05    0.00    0.05    0.18\n",
       "beta[32]          -0.84    0.00    0.10   -1.03   -0.90   -0.83   -0.77   -0.65\n",
       "beta[33]          -1.80    0.00    0.10   -1.99   -1.87   -1.80   -1.74   -1.61\n",
       "beta[34]          -0.40    0.00    0.10   -0.60   -0.47   -0.40   -0.34   -0.21\n",
       "beta[35]          -1.51    0.00    0.10   -1.70   -1.58   -1.51   -1.44   -1.32\n",
       "beta[36]          -0.20    0.00    0.10   -0.40   -0.27   -0.20   -0.14   -0.01\n",
       "beta[37]          -1.21    0.00    0.10   -1.41   -1.28   -1.21   -1.14   -1.01\n",
       "beta[38]          10.99    0.00    0.10   10.79   10.92   10.99   11.05   11.19\n",
       "beta[39]          -0.40    0.00    0.11   -0.60   -0.47   -0.39   -0.32   -0.19\n",
       "beta[40]           1.26    0.00    0.10    1.06    1.19    1.26    1.33    1.46\n",
       "beta[41]           0.50    0.00    0.11    0.29    0.43    0.50    0.57    0.70\n",
       "beta[42]           0.46    0.00    0.11    0.24    0.39    0.47    0.54    0.67\n",
       "beta[43]           1.32    0.00    0.10    1.14    1.25    1.32    1.38    1.51\n",
       "beta[44]           0.88    0.00    0.10    0.69    0.82    0.88    0.95    1.07\n",
       "beta[45]           0.41    0.00    0.11    0.20    0.34    0.41    0.49    0.63\n",
       "beta[46]          -1.59    0.00    0.10   -1.80   -1.66   -1.59   -1.53   -1.39\n",
       "beta[47]          -0.60    0.00    0.08   -0.76   -0.65   -0.60   -0.54   -0.43\n",
       "beta[48]           0.44    0.00    0.11    0.23    0.37    0.44    0.51    0.65\n",
       "beta[49]           0.12    0.00    0.12   -0.09    0.03    0.12    0.20    0.36\n",
       "beta[50]          -0.64    0.00    0.10   -0.82   -0.71   -0.64   -0.57   -0.44\n",
       "beta[51]          -0.52    0.00    0.11   -0.73   -0.59   -0.52   -0.45   -0.31\n",
       "beta[52]          -0.52    0.00    0.09   -0.70   -0.58   -0.52   -0.46   -0.34\n",
       "beta[53]          -0.60    0.00    0.10   -0.79   -0.67   -0.60   -0.53   -0.40\n",
       "beta[54]          -0.15    0.00    0.11   -0.36   -0.22   -0.15   -0.07    0.03\n",
       "beta[55]           1.39    0.00    0.11    1.18    1.32    1.39    1.46    1.60\n",
       "beta[56]          -1.22    0.00    0.10   -1.42   -1.28   -1.22   -1.15   -1.02\n",
       "beta[57]          -0.33    0.00    0.10   -0.53   -0.40   -0.33   -0.26   -0.13\n",
       "beta[58]           2.49    0.00    0.12    2.27    2.41    2.49    2.57    2.72\n",
       "beta[59]          -0.91    0.00    0.11   -1.12   -0.98   -0.91   -0.84   -0.69\n",
       "beta[60]          -1.77    0.00    0.10   -1.96   -1.84   -1.77   -1.71   -1.58\n",
       "beta[61]           0.84    0.00    0.09    0.66    0.78    0.85    0.90    1.03\n",
       "beta[62]           0.06    0.00    0.09   -0.10    0.00    0.05    0.12    0.25\n",
       "beta[63]          -0.74    0.00    0.11   -0.94   -0.81   -0.74   -0.66   -0.51\n",
       "beta[64]           0.90    0.00    0.10    0.70    0.84    0.90    0.97    1.10\n",
       "beta[65]          -1.11    0.00    0.10   -1.31   -1.17   -1.11   -1.04   -0.91\n",
       "beta[66]          -0.48    0.00    0.10   -0.68   -0.55   -0.48   -0.41   -0.27\n",
       "beta[67]           8.40    0.00    0.11    8.19    8.33    8.39    8.47    8.61\n",
       "beta[68]           0.83    0.00    0.09    0.64    0.76    0.83    0.89    1.00\n",
       "beta[69]           1.07    0.00    0.09    0.88    1.00    1.07    1.13    1.25\n",
       "beta[70]          -0.16    0.00    0.09   -0.35   -0.23   -0.17   -0.10    0.01\n",
       "beta[71]          -0.12    0.00    0.10   -0.33   -0.19   -0.12   -0.05    0.05\n",
       "beta[72]          -0.49    0.00    0.10   -0.70   -0.56   -0.49   -0.43   -0.29\n",
       "beta[73]           0.34    0.00    0.10    0.14    0.27    0.34    0.41    0.54\n",
       "beta[74]           1.17    0.00    0.11    0.96    1.09    1.16    1.24    1.37\n",
       "beta[75]           0.12    0.00    0.09   -0.04    0.05    0.11    0.18    0.29\n",
       "beta[76]           2.19    0.00    0.11    1.98    2.13    2.19    2.26    2.40\n",
       "beta[77]          -1.35    0.00    0.10   -1.56   -1.42   -1.36   -1.28   -1.14\n",
       "beta[78]           0.66    0.00    0.11    0.45    0.59    0.66    0.73    0.87\n",
       "beta[79]          -1.62    0.00    0.10   -1.81   -1.68   -1.62   -1.55   -1.43\n",
       "beta[80]           1.74    0.00    0.10    1.54    1.67    1.75    1.81    1.95\n",
       "beta[81]           1.63    0.00    0.09    1.45    1.57    1.64    1.70    1.81\n",
       "beta[82]         -11.03    0.00    0.10  -11.22  -11.10  -11.03  -10.97  -10.84\n",
       "beta[83]          -1.16    0.00    0.10   -1.36   -1.23   -1.16   -1.10   -0.97\n",
       "beta[84]           0.77    0.00    0.11    0.56    0.70    0.77    0.84    0.97\n",
       "beta[85]          -0.04    0.00    0.08   -0.21   -0.09   -0.03    0.01    0.11\n",
       "beta[86]           0.59    0.00    0.10    0.39    0.53    0.59    0.66    0.79\n",
       "beta[87]           0.76    0.00    0.10    0.56    0.69    0.76    0.82    0.96\n",
       "beta[88]          -0.91    0.00    0.11   -1.13   -0.98   -0.91   -0.84   -0.70\n",
       "beta[89]          -0.65    0.00    0.10   -0.86   -0.72   -0.65   -0.59   -0.46\n",
       "beta[90]           0.60    0.00    0.11    0.39    0.53    0.60    0.68    0.83\n",
       "beta[91]          -0.71    0.00    0.11   -0.92   -0.78   -0.71   -0.63   -0.48\n",
       "beta[92]           1.13    0.00    0.09    0.95    1.07    1.13    1.19    1.31\n",
       "beta[93]          -1.22    0.00    0.10   -1.40   -1.28   -1.22   -1.15   -1.02\n",
       "beta[94]          -0.34    0.00    0.10   -0.54   -0.41   -0.35   -0.28   -0.15\n",
       "beta[95]          -0.84    0.00    0.09   -1.02   -0.90   -0.84   -0.78   -0.66\n",
       "beta[96]          -0.69    0.00    0.11   -0.91   -0.77   -0.69   -0.61   -0.46\n",
       "beta[97]           2.63    0.00    0.09    2.46    2.57    2.63    2.69    2.81\n",
       "beta[98]          -1.54    0.00    0.09   -1.72   -1.60   -1.54   -1.47   -1.35\n",
       "beta[99]          -0.22    0.00    0.10   -0.41   -0.28   -0.22   -0.15   -0.01\n",
       "beta[100]          0.49    0.00    0.09    0.32    0.43    0.49    0.55    0.66\n",
       "lp__            -297.81    0.49   14.33 -327.06 -306.95 -297.19 -288.04 -271.06\n",
       "                n_eff Rhat\n",
       "beta_tilde[1]    3375    1\n",
       "beta_tilde[2]    3514    1\n",
       "beta_tilde[3]    3125    1\n",
       "beta_tilde[4]    4286    1\n",
       "beta_tilde[5]    4869    1\n",
       "beta_tilde[6]    2547    1\n",
       "beta_tilde[7]    3283    1\n",
       "beta_tilde[8]    2995    1\n",
       "beta_tilde[9]    4735    1\n",
       "beta_tilde[10]   3160    1\n",
       "beta_tilde[11]   5026    1\n",
       "beta_tilde[12]   5391    1\n",
       "beta_tilde[13]   3861    1\n",
       "beta_tilde[14]   2870    1\n",
       "beta_tilde[15]   3750    1\n",
       "beta_tilde[16]   4711    1\n",
       "beta_tilde[17]   3587    1\n",
       "beta_tilde[18]   4894    1\n",
       "beta_tilde[19]   5163    1\n",
       "beta_tilde[20]   3838    1\n",
       "beta_tilde[21]   4730    1\n",
       "beta_tilde[22]   4746    1\n",
       "beta_tilde[23]   4086    1\n",
       "beta_tilde[24]   4187    1\n",
       "beta_tilde[25]   4067    1\n",
       "beta_tilde[26]   4058    1\n",
       "beta_tilde[27]   3277    1\n",
       "beta_tilde[28]   4213    1\n",
       "beta_tilde[29]   4521    1\n",
       "beta_tilde[30]   3282    1\n",
       "beta_tilde[31]   3145    1\n",
       "beta_tilde[32]   5361    1\n",
       "beta_tilde[33]   4950    1\n",
       "beta_tilde[34]   4231    1\n",
       "beta_tilde[35]   4620    1\n",
       "beta_tilde[36]   3404    1\n",
       "beta_tilde[37]   5253    1\n",
       "beta_tilde[38]   3361    1\n",
       "beta_tilde[39]   3271    1\n",
       "beta_tilde[40]   4511    1\n",
       "beta_tilde[41]   3885    1\n",
       "beta_tilde[42]   3865    1\n",
       "beta_tilde[43]   4874    1\n",
       "beta_tilde[44]   4758    1\n",
       "beta_tilde[45]   4045    1\n",
       "beta_tilde[46]   4811    1\n",
       "beta_tilde[47]   4827    1\n",
       "beta_tilde[48]   4079    1\n",
       "beta_tilde[49]   3229    1\n",
       "beta_tilde[50]   5103    1\n",
       "beta_tilde[51]   4065    1\n",
       "beta_tilde[52]   3624    1\n",
       "beta_tilde[53]   4507    1\n",
       "beta_tilde[54]   3036    1\n",
       "beta_tilde[55]   4709    1\n",
       "beta_tilde[56]   3889    1\n",
       "beta_tilde[57]   3715    1\n",
       "beta_tilde[58]   4430    1\n",
       "beta_tilde[59]   4608    1\n",
       "beta_tilde[60]   5641    1\n",
       "beta_tilde[61]   4616    1\n",
       "beta_tilde[62]   3475    1\n",
       "beta_tilde[63]   4786    1\n",
       "beta_tilde[64]   5576    1\n",
       "beta_tilde[65]   4962    1\n",
       "beta_tilde[66]   4519    1\n",
       "beta_tilde[67]   2987    1\n",
       "beta_tilde[68]   4900    1\n",
       "beta_tilde[69]   4641    1\n",
       "beta_tilde[70]   3353    1\n",
       "beta_tilde[71]   3356    1\n",
       "beta_tilde[72]   4618    1\n",
       "beta_tilde[73]   3564    1\n",
       "beta_tilde[74]   4586    1\n",
       "beta_tilde[75]   3627    1\n",
       "beta_tilde[76]   4279    1\n",
       "beta_tilde[77]   4561    1\n",
       "beta_tilde[78]   4782    1\n",
       "beta_tilde[79]   5196    1\n",
       "beta_tilde[80]   4933    1\n",
       "beta_tilde[81]   4589    1\n",
       "beta_tilde[82]   3118    1\n",
       "beta_tilde[83]   4628    1\n",
       "beta_tilde[84]   4772    1\n",
       "beta_tilde[85]   3973    1\n",
       "beta_tilde[86]   4609    1\n",
       "beta_tilde[87]   4553    1\n",
       "beta_tilde[88]   4491    1\n",
       "beta_tilde[89]   4356    1\n",
       "beta_tilde[90]   4706    1\n",
       "beta_tilde[91]   4337    1\n",
       "beta_tilde[92]   4460    1\n",
       "beta_tilde[93]   4475    1\n",
       "beta_tilde[94]   3873    1\n",
       "beta_tilde[95]   4577    1\n",
       "beta_tilde[96]   4467    1\n",
       "beta_tilde[97]   4871    1\n",
       "beta_tilde[98]   4958    1\n",
       "beta_tilde[99]   2632    1\n",
       "beta_tilde[100]  3748    1\n",
       "lambda[1]        3824    1\n",
       "lambda[2]        3917    1\n",
       "lambda[3]        2080    1\n",
       "lambda[4]        3126    1\n",
       "lambda[5]        3575    1\n",
       "lambda[6]        2852    1\n",
       "lambda[7]        3951    1\n",
       "lambda[8]        2878    1\n",
       "lambda[9]        3843    1\n",
       "lambda[10]       2612    1\n",
       "lambda[11]       2989    1\n",
       "lambda[12]       2075    1\n",
       "lambda[13]       3936    1\n",
       "lambda[14]       3833    1\n",
       "lambda[15]       2142    1\n",
       "lambda[16]       3271    1\n",
       "lambda[17]       4009    1\n",
       "lambda[18]       3298    1\n",
       "lambda[19]       2659    1\n",
       "lambda[20]       3154    1\n",
       "lambda[21]       3913    1\n",
       "lambda[22]       2798    1\n",
       "lambda[23]       3456    1\n",
       "lambda[24]       2534    1\n",
       "lambda[25]       2789    1\n",
       "lambda[26]       3404    1\n",
       "lambda[27]       3965    1\n",
       "lambda[28]       2140    1\n",
       "lambda[29]       2613    1\n",
       "lambda[30]       2829    1\n",
       "lambda[31]       3237    1\n",
       "lambda[32]       3826    1\n",
       "lambda[33]       3956    1\n",
       "lambda[34]       3762    1\n",
       "lambda[35]       2023    1\n",
       "lambda[36]       2273    1\n",
       "lambda[37]       2007    1\n",
       "lambda[38]       3052    1\n",
       "lambda[39]       3803    1\n",
       "lambda[40]       3935    1\n",
       "lambda[41]       3619    1\n",
       "lambda[42]       2602    1\n",
       "lambda[43]       2409    1\n",
       "lambda[44]       3724    1\n",
       "lambda[45]       2772    1\n",
       "lambda[46]       3013    1\n",
       "lambda[47]       3479    1\n",
       "lambda[48]       1949    1\n",
       "lambda[49]       3386    1\n",
       "lambda[50]       2296    1\n",
       "lambda[51]       3943    1\n",
       "lambda[52]       2402    1\n",
       "lambda[53]       2146    1\n",
       "lambda[54]       3287    1\n",
       "lambda[55]       2469    1\n",
       "lambda[56]       2651    1\n",
       "lambda[57]       2449    1\n",
       "lambda[58]       1706    1\n",
       "lambda[59]       3011    1\n",
       "lambda[60]       2713    1\n",
       "lambda[61]       2276    1\n",
       "lambda[62]       2276    1\n",
       "lambda[63]       1907    1\n",
       "lambda[64]       3041    1\n",
       "lambda[65]       2103    1\n",
       "lambda[66]       2786    1\n",
       "lambda[67]       2331    1\n",
       "lambda[68]       3106    1\n",
       "lambda[69]       1081    1\n",
       "lambda[70]       3698    1\n",
       "lambda[71]       3431    1\n",
       "lambda[72]       2319    1\n",
       "lambda[73]       3921    1\n",
       "lambda[74]       1959    1\n",
       "lambda[75]       3699    1\n",
       "lambda[76]       2272    1\n",
       "lambda[77]       3085    1\n",
       "lambda[78]       3497    1\n",
       "lambda[79]       2824    1\n",
       "lambda[80]       2311    1\n",
       "lambda[81]       3865    1\n",
       "lambda[82]       3902    1\n",
       "lambda[83]       2425    1\n",
       "lambda[84]       2944    1\n",
       "lambda[85]       3198    1\n",
       "lambda[86]       3229    1\n",
       "lambda[87]       2898    1\n",
       "lambda[88]       2355    1\n",
       "lambda[89]       3974    1\n",
       "lambda[90]       3099    1\n",
       "lambda[91]       3428    1\n",
       "lambda[92]       3432    1\n",
       "lambda[93]       3162    1\n",
       "lambda[94]       1535    1\n",
       "lambda[95]       2757    1\n",
       "lambda[96]       2600    1\n",
       "lambda[97]       2950    1\n",
       "lambda[98]       2816    1\n",
       "lambda[99]       3154    1\n",
       "lambda[100]      3828    1\n",
       "c2_tilde         2879    1\n",
       "tau_tilde        2527    1\n",
       "alpha            7560    1\n",
       "sigma            1638    1\n",
       "beta[1]          3984    1\n",
       "beta[2]          4169    1\n",
       "beta[3]          3690    1\n",
       "beta[4]          3741    1\n",
       "beta[5]          3672    1\n",
       "beta[6]          3411    1\n",
       "beta[7]          3574    1\n",
       "beta[8]          3991    1\n",
       "beta[9]          3854    1\n",
       "beta[10]         4129    1\n",
       "beta[11]         4338    1\n",
       "beta[12]         3651    1\n",
       "beta[13]         4522    1\n",
       "beta[14]         3616    1\n",
       "beta[15]         3860    1\n",
       "beta[16]         3969    1\n",
       "beta[17]         3515    1\n",
       "beta[18]         4109    1\n",
       "beta[19]         4062    1\n",
       "beta[20]         3904    1\n",
       "beta[21]         3968    1\n",
       "beta[22]         4097    1\n",
       "beta[23]         3850    1\n",
       "beta[24]         3737    1\n",
       "beta[25]         3422    1\n",
       "beta[26]         3930    1\n",
       "beta[27]         3290    1\n",
       "beta[28]         3875    1\n",
       "beta[29]         3915    1\n",
       "beta[30]         4154    1\n",
       "beta[31]         3743    1\n",
       "beta[32]         4103    1\n",
       "beta[33]         4045    1\n",
       "beta[34]         4170    1\n",
       "beta[35]         4101    1\n",
       "beta[36]         2833    1\n",
       "beta[37]         3707    1\n",
       "beta[38]         3932    1\n",
       "beta[39]         4111    1\n",
       "beta[40]         3888    1\n",
       "beta[41]         4083    1\n",
       "beta[42]         3914    1\n",
       "beta[43]         4326    1\n",
       "beta[44]         3264    1\n",
       "beta[45]         3948    1\n",
       "beta[46]         3861    1\n",
       "beta[47]         4158    1\n",
       "beta[48]         3566    1\n",
       "beta[49]         3857    1\n",
       "beta[50]         4228    1\n",
       "beta[51]         4345    1\n",
       "beta[52]         3952    1\n",
       "beta[53]         3832    1\n",
       "beta[54]         3604    1\n",
       "beta[55]         4089    1\n",
       "beta[56]         3940    1\n",
       "beta[57]         4201    1\n",
       "beta[58]         3486    1\n",
       "beta[59]         3935    1\n",
       "beta[60]         3695    1\n",
       "beta[61]         3785    1\n",
       "beta[62]         3333    1\n",
       "beta[63]         3662    1\n",
       "beta[64]         3646    1\n",
       "beta[65]         4093    1\n",
       "beta[66]         3958    1\n",
       "beta[67]         4068    1\n",
       "beta[68]         3944    1\n",
       "beta[69]         4062    1\n",
       "beta[70]         2915    1\n",
       "beta[71]         3854    1\n",
       "beta[72]         4180    1\n",
       "beta[73]         3893    1\n",
       "beta[74]         3491    1\n",
       "beta[75]         3073    1\n",
       "beta[76]         4026    1\n",
       "beta[77]         4123    1\n",
       "beta[78]         3989    1\n",
       "beta[79]         4094    1\n",
       "beta[80]         3840    1\n",
       "beta[81]         3503    1\n",
       "beta[82]         3953    1\n",
       "beta[83]         3388    1\n",
       "beta[84]         3989    1\n",
       "beta[85]         4074    1\n",
       "beta[86]         3974    1\n",
       "beta[87]         3840    1\n",
       "beta[88]         3910    1\n",
       "beta[89]         4073    1\n",
       "beta[90]         3712    1\n",
       "beta[91]         3814    1\n",
       "beta[92]         3547    1\n",
       "beta[93]         3918    1\n",
       "beta[94]         3546    1\n",
       "beta[95]         3923    1\n",
       "beta[96]         3790    1\n",
       "beta[97]         3725    1\n",
       "beta[98]         4266    1\n",
       "beta[99]         3648    1\n",
       "beta[100]        4004    1\n",
       "lp__              841    1\n",
       "\n",
       "Samples were drawn using NUTS(diag_e) at Mon Nov 25 23:29:24 2019.\n",
       "For each parameter, n_eff is a crude measure of effective sample size,\n",
       "and Rhat is the potential scale reduction factor on split chains (at \n",
       "convergence, Rhat=1)."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finnish_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
