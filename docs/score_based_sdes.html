<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.38">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Simon Dirmeier">

<title>Diffusion models II: DDPMs, NCSNs and score-based generative modelling using SDEs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="score_based_sdes_files/libs/clipboard/clipboard.min.js"></script>
<script src="score_based_sdes_files/libs/quarto-html/quarto.js"></script>
<script src="score_based_sdes_files/libs/quarto-html/popper.min.js"></script>
<script src="score_based_sdes_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="score_based_sdes_files/libs/quarto-html/anchor.min.js"></script>
<link href="score_based_sdes_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="score_based_sdes_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="score_based_sdes_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="score_based_sdes_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="score_based_sdes_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc">
   
  <ul>
  <li><a href="#ddpms" id="toc-ddpms" class="nav-link active" data-scroll-target="#ddpms">DDPMs</a></li>
  <li><a href="#ncsns" id="toc-ncsns" class="nav-link" data-scroll-target="#ncsns">NCSNs</a></li>
  <li><a href="#score-based-sdes" id="toc-score-based-sdes" class="nav-link" data-scroll-target="#score-based-sdes">Score-based SDEs</a>
  <ul class="collapse">
  <li><a href="#similarities" id="toc-similarities" class="nav-link" data-scroll-target="#similarities">Similarities</a></li>
  <li><a href="#a-common-framework" id="toc-a-common-framework" class="nav-link" data-scroll-target="#a-common-framework">A common framework</a></li>
  </ul></li>
  <li><a href="#use-cases" id="toc-use-cases" class="nav-link" data-scroll-target="#use-cases">Use cases</a>
  <ul class="collapse">
  <li><a href="#score-model" id="toc-score-model" class="nav-link" data-scroll-target="#score-model">Score model</a></li>
  <li><a href="#ncsn" id="toc-ncsn" class="nav-link" data-scroll-target="#ncsn">NCSN</a></li>
  <li><a href="#ddpm" id="toc-ddpm" class="nav-link" data-scroll-target="#ddpm">DDPM</a></li>
  <li><a href="#score-based-sde" id="toc-score-based-sde" class="nav-link" data-scroll-target="#score-based-sde">Score-based SDE</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#session-info" id="toc-session-info" class="nav-link" data-scroll-target="#session-info">Session info</a></li>
  <li><a href="#license" id="toc-license" class="nav-link" data-scroll-target="#license">License</a>
  <ul class="collapse">
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Diffusion models II: DDPMs, NCSNs and score-based generative modelling using SDEs</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Simon Dirmeier </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January, 2023</p>
    </div>
  </div>
    
  </div>
  

</header>

<p>The following case study introduces recent developments in generative modelling using diffusion models. We will first introduce two landmark papers on diffusion models, “Denoising Diffusion Probabilistic Models” <span class="citation" data-cites="ho2020denoising">(<a href="#ref-ho2020denoising" role="doc-biblioref">Ho, Jain, and Abbeel 2020</a>)</span> and “Generative Modeling by Estimating Gradients of the Data Distribution” <span class="citation" data-cites="song2019generative">(<a href="#ref-song2019generative" role="doc-biblioref">Song and Ermon 2019</a>)</span>, and then examine a general framework introduced in <span class="citation" data-cites="song2021scorebased">Song et al. (<a href="#ref-song2021scorebased" role="doc-biblioref">2021</a>)</span> that unifies both. This case study is the second in a series on diffusion models: please find the first one <a href="https://dirmeier.github.io/etudes/diffusion_models.html">here</a>.</p>
<p>We’ll reimplement all three vanilla models using Jax, Haiku and Optax.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> distrax</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> haiku <span class="im">as</span> hk</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optax</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> lax, nn, jit, grad</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> numpy <span class="im">as</span> jnp</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> random</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> scipy <span class="im">as</span> jsp</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> integrate</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> palettes</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(rc<span class="op">=</span>{<span class="st">"figure.figsize"</span>: (<span class="dv">6</span>, <span class="dv">3</span>)})</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"ticks"</span>, {<span class="st">"font.family"</span>: <span class="st">"serif"</span>, <span class="st">"font.serif"</span>: <span class="st">"Merriweather"</span>})</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>palettes.set_theme()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="ddpms" class="level1">
<h1>DDPMs</h1>
<p>As discussed in <a href="https://dirmeier.github.io/etudes/diffusion_models.html">the first part of the series</a>, diffusion models <span class="citation" data-cites="sohl2015deep">(<a href="#ref-sohl2015deep" role="doc-biblioref">Sohl-Dickstein et al. 2015</a>)</span> are a class of generative models of the form</p>
<p><span class="math display">\[
p_\theta \left( \mathbf{y}_0 \right)  = \int p_\theta \left( \mathbf{y}_0, \mathbf{y}_{1:T} \right) d\mathbf{y}_{1:T}
\]</span></p>
<p>where every transition <span class="math inline">\(p_\theta(\mathbf{y}_{t-1} \mid \mathbf{y}_{t})\)</span> is parameterized by the same neural network <span class="math inline">\(\theta\)</span> and trained by optimizing a lower bound on the marginal log-likelihood:</p>
<p><span class="math display">\[\begin{align*}
\log p_\theta(\mathbf{y}_0) \ge \mathbb{E}_{q(\mathbf{y}_{1:T})}
\Bigl[
&amp; \log p_\theta(\mathbf{y}_0 \mid \mathbf{y}_{1}) \\
&amp;+ \sum_{i=1}^{T - 1} \log p_\theta(\mathbf{y}_i \mid \mathbf{y}_{i + 1}) - \sum_{i=2}^{T} \log q(\mathbf{y}_i \mid \mathbf{y}_{i - 1})  \\
&amp; +  \log p_\theta(\mathbf{y}_T) - \log q(\mathbf{y}_1 \mid \mathbf{y}_0)
\Bigr]
\end{align*}\]</span></p>
<p>In comparison to other latent variables models, DPMs use an approximate posterior <span class="math inline">\(q\left(\mathbf{y}_{1:T} \mid \mathbf{y}_0 \right) = q(\mathbf{y}_{1} \mid \mathbf{y}_0) \prod_{t=2}^T q(\mathbf{y}_{t} \mid \mathbf{y}_{t - 1})\)</span> that is fixed to a Markov chain and which repeatedly adds noise to the initial data set, while the joint distribution <span class="math inline">\(p_\theta \left( \mathbf{y}_0, \mathbf{y}_{1:T} \right)\)</span> is learned. As discussed in the previous case study, the ELBO above can be reformulated as</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}_{q} \biggl[
\log p_\theta \left(\mathbf{y}_0 \mid \mathbf{y}_1 \right) -
\sum_{t=2}^T \mathbb{KL}\Bigl[ q(\mathbf{y}_{t - 1} \mid \mathbf{y}_{t}, \mathbf{y}_0), p_\theta(\mathbf{y}_{t - 1} \mid \mathbf{y}_t)  \Bigr] -
\mathbb{KL}\Bigl[ q(\mathbf{y}_T \mid \mathbf{y}_0), p_\theta(\mathbf{y}_T)  \Bigr]
\biggr]
\end{align*}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align*}
q \left(  \mathbf{y}_{t - 1} \mid  \mathbf{y}_{t}, \mathbf{y}_0 \right)  &amp;  =  
\frac{q \left(  \mathbf{y}_{t} \mid  \mathbf{y}_{t-1} , \mathbf{y}_0 \right) q\left(  \mathbf{y}_{t-1} \mid  \mathbf{y}_0 \right)  }{q \left(  \mathbf{y}_{t} \mid  \mathbf{y}_0 \right) } \\
&amp;  = \mathcal{N} \left( \tilde{\boldsymbol \mu}_t\left( \mathbf{y}_{t}, \mathbf{y}_0\right) , \tilde{\beta}_t \mathbf{I} \right)
\end{align*}\]</span></p>
<p>which can be computed in closed form. The insight from <span class="citation" data-cites="ho2020denoising">Ho, Jain, and Abbeel (<a href="#ref-ho2020denoising" role="doc-biblioref">2020</a>)</span> is that the objective above can be simplified (and numerically stabilized) by setting the covariance matrix of the reverse process <span class="math inline">\(p_\theta \left(\mathbf{y}_{t-1} \mid \mathbf{y}_{t} \right) = \mathcal{N}\left(\boldsymbol \mu_\theta \left(\mathbf{y}_{t}, t\right), \boldsymbol \Sigma(\mathbf{y}_{t}, t)\right)\)</span> to a constant <span class="math inline">\(\boldsymbol \Sigma \left(\mathbf{y}_{t}, t \right) = \sigma_t^2\mathbf{I}\)</span>. The divergence between the forward process posterior and the reverse process can then be written as</p>
<p><span class="math display">\[\begin{align*}
L_{t-1}  &amp;  =  
\mathbb{E}_{q} \biggl[
\frac{1}{2\sigma_t^2}
\| \tilde{\boldsymbol \mu}_t\left(\mathbf{y}_t, \mathbf{y}_0\right) - \boldsymbol \mu_{\theta}\left(\mathbf{y}_t, \mathbf{y_0}\right)    \|^2_2
\biggr] + C
\end{align*}\]</span></p>
<p>So, we can parameterize <span class="math inline">\(\boldsymbol \mu_\theta\)</span> using a model that predicts the forward process posterior mean <span class="math inline">\(\tilde{\boldsymbol \mu}\)</span>. We can further develop this formulation by applying the usual Gaussian reparameterization trick and arrive at an objective that is significantly more stable to optimize than the initial ELBO, but for the sake of this case study we will not derive the math here and refer the read to the original publication.</p>
<p>In practice, we just sample a time point <span class="math inline">\(t\)</span> and then don’t optimze the entire objective but only</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}_{t, q} \biggl[
\frac{1}{2\sigma_t^2}
\| \tilde{\boldsymbol \mu}_t\left(\mathbf{y}_t, \mathbf{y}_0\right) - \boldsymbol \mu_{\theta}\left(\mathbf{y}_t, \mathbf{y}_0\right)    \|^2_2
\biggr]
\end{align*}\]</span></p>
<p>where we denote - with a slight abuse of notation - <span class="math inline">\(t\)</span> as both a time point and the distribution of time points over which the expectation is taken.</p>
<p>To generate new data, we sample first from the prior <span class="math inline">\(p_T\)</span> and then iteratively denoise the sample until we reach <span class="math inline">\(p_\theta(\mathbf{y}_0 \mid \mathbf{y}_1)\)</span>.</p>
</section>
<section id="ncsns" class="level1">
<h1>NCSNs</h1>
<p>Similarly to DDPMs, noise-conditional score networks (NCSNs, <span class="citation" data-cites="song2019generative">Song and Ermon (<a href="#ref-song2019generative" role="doc-biblioref">2019</a>)</span>) are generative models that make use of noising the data and then trying to train a network that can denoise this process. Unlike DDPMs, NCSNs are motivated through score-matching <span class="citation" data-cites="hyvarinen2005estimation">(<a href="#ref-hyvarinen2005estimation" role="doc-biblioref">Hyvärinen and Dayan 2005</a>)</span> where we are interested in finding a parameterized function <span class="math inline">\(\mathbf{s}_\theta\)</span> that approximates the score of a data distribution <span class="math inline">\(q(\mathbf{y})\)</span>. Score matching optimizes the following objective</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}_{q(\mathbf{y})} \biggl[
\| \mathbf{s}_\theta(\mathbf{y}) - \nabla_{\mathbf{y}} \log q(\mathbf{y})
||^2_2
\biggr]
\end{align*}\]</span></p>
<p>In denoising score matching <span class="citation" data-cites="vincent2011connection">(<a href="#ref-vincent2011connection" role="doc-biblioref">Vincent 2011</a>)</span>, we denoise a data sample with a fixed noise distribution <span class="math inline">\(q_\sigma \left(\tilde{\mathbf{y}} \mid \mathbf{y} \right) = \mathcal{N}\left(\mathbf{y}, \sigma^2 \mathbf{I} \right)\)</span> to estimate the score of <span class="math inline">\(q_\sigma \left(\tilde{\mathbf{y}} \right) = \int q_\sigma \left(\tilde{\mathbf{y}} \mid \mathbf{y} \right) p(\mathbf{y})\)</span>. <span class="citation" data-cites="vincent2011connection">Vincent (<a href="#ref-vincent2011connection" role="doc-biblioref">2011</a>)</span> show that the objective proved equivalent to:</p>
<p><span class="math display">\[\begin{align*}
\frac{1}{2}\mathbb{E}_{ q(\mathbf{y}), q_\sigma \left(\tilde{\mathbf{y}} \mid \mathbf{y} \right)} \biggl[
\| \mathbf{s}_\theta\left(\tilde{\mathbf{y}}\right) - \nabla_{\tilde{\mathbf{y}}} \log q_\sigma \left(\tilde{\mathbf{y}} \mid \mathbf{y} \right)
||^2_2
\biggr]
\end{align*}\]</span></p>
<p>In practice, generative modelling using this objective faces two difficulties. One is that if <span class="math inline">\(\mathbf{y}\)</span> is embedded in a low-dimensional manifold, the gradient taken in the ambient space is undefined. The score estimator is not consistent if the data are residing in a low-dimensional space. Secondly, for regions where there is low data density the score estimator is not accurate.</p>
<p><span class="citation" data-cites="song2019generative">Song and Ermon (<a href="#ref-song2019generative" role="doc-biblioref">2019</a>)</span> work around both issues by perturbing the data set with multiple levels of noise <span class="math inline">\(\{ \sigma_i \}_{i=1}^L\)</span> and training a score network that is conditional on the noise levels <span class="math inline">\(\mathbf{s}\left(\tilde{\mathbf{y}}, \sigma_i \right)\)</span>. The authors propose the following objective to train a model of the data:</p>
<p><span class="math display">\[\begin{align*}
\frac{1}{2}\mathbb{E}_{\sigma_i, q(\mathbf{y}), q_\sigma \left(\tilde{\mathbf{y}} \mid \mathbf{y} \right)} \biggl[\lambda(\sigma_i)
\| \mathbf{s}_\theta\left(\tilde{\mathbf{y}}, \sigma_i\right) - \nabla_{\tilde{\mathbf{y}}} \log q_\sigma \left(\tilde{\mathbf{y}} \mid \mathbf{y} \right)
||^2_2
\biggr]
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\lambda(\sigma_i)\)</span> is a weighting function that is chosen to be proportional to <span class="math inline">\(\lambda(\sigma_i) \propto 1 / \mathbb{E}\left[ \|\nabla_{\mathbf{y}} q_\sigma \left(\tilde{\mathbf{y}} \mid \mathbf{y} \|^2_2 \right) \right]\)</span>.</p>
<p>To draw samples from the trained model, <span class="citation" data-cites="song2019generative">Song and Ermon (<a href="#ref-song2019generative" role="doc-biblioref">2019</a>)</span> propose using annealed Langevin dynamics where we only need access to the score function of a density to propose new samples (which we just learned).</p>
</section>
<section id="score-based-sdes" class="level1">
<h1>Score-based SDEs</h1>
<p>Before we introduce score-based SDEs, let’s have a look at the DDPM and NCSN objectives again.</p>
<section id="similarities" class="level2">
<h2 class="anchored" data-anchor-id="similarities">Similarities</h2>
<p>Let’s first recall that in the DDPM framework we parameterize a model using a neural network <span class="math inline">\(\boldsymbol \mu_\theta \left(\mathbf{y}_t, t \right)\)</span> to predict the forward process posterior mean <span class="math inline">\(\tilde{\boldsymbol \mu}\left(\mathbf{y}_t, \mathbf{y}_0 \right) =\frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{y}_t + \frac{\sqrt{\bar{\alpha}_{t-1} } \beta_t}{1 - \bar{\alpha}_{t}} \mathbf{y}_0\)</span>. We recognize that since we actually have access to <span class="math inline">\(\mathbf{y}_t\)</span>, we can <em>alternatively</em> make <span class="math inline">\(\boldsymbol \mu_\theta \left(\mathbf{y}_t, t \right)\)</span> predict <span class="math inline">\(\mathbf{y}_0\)</span>.</p>
<p>Now, if we recall that <span class="math inline">\(q(\mathbf{y}_t \mid \mathbf{y}_0) = \mathcal{N}\left(\sqrt{\bar{\alpha_t}} \mathbf{y}_0, \left(1 - \bar{\alpha}_t \right) \mathbf{I} \right)\)</span> and compute the partial derivate of its logarithm w.r.t <span class="math inline">\(\mathbf{y}_t\)</span></p>
<p><span class="math display">\[\begin{align*}
\nabla_{\mathbf{y}_t} \log q(\mathbf{y}_t \mid \mathbf{y}_0) = - \frac{\mathbf{y}_t - \sqrt{\bar{\alpha_t}} \mathbf{y}_0}{1 - \bar{\alpha}_t}
\end{align*}\]</span></p>
<p>we can see that we can, at least with some algebraic gymnastics, equivalently write</p>
<p><span class="math display">\[\begin{align*}
\tilde{\boldsymbol \mu}\left(\mathbf{y}_t, t \right) &amp;= \frac{1}{\sqrt{\alpha_t}} \mathbf{y}_t  + \frac{\beta_t}{\sqrt{\alpha_t}} \nabla_{\mathbf{y}_t} \log q(\mathbf{y}_t \mid \mathbf{y}_0) \\
&amp; = \frac{1}{\sqrt{\alpha_t}} \mathbf{y}_t - \frac{\beta_t}{\sqrt{\alpha_t}} \left( \frac{\mathbf{y}_t - \sqrt{\bar{\alpha}_t}\mathbf{y}_0 }{1 - \bar{\alpha}_t} \right) \\
&amp; =  \frac{1}{\sqrt{\alpha_t}} \mathbf{y}_t - \frac{\beta_t \mathbf{y}_t}{\sqrt{\alpha_t}\left( 1 - \bar{\alpha}_t\right)} + \frac{\beta_t \sqrt{\bar{\alpha}_t}  \mathbf{y}_0}{\sqrt{\alpha_t} \left( 1 - \bar{\alpha}_t\right) } \\
&amp; =  \frac{\left( 1 - \bar{\alpha}_t\right)\mathbf{y}_t - \beta_t \mathbf{y}_t}{\sqrt{\alpha_t}\left( 1 - \bar{\alpha}_t\right)} + \frac{\beta_t \sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_t}\mathbf{y}_0 \\
&amp; =  \frac{\left( 1 - \bar{\alpha}_t - \beta_t\right)\mathbf{y}_t}{\sqrt{\alpha_t}\left( 1 - \bar{\alpha}_t\right)} + \frac{\beta_t \sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_t}\mathbf{y}_0 \\
&amp; =  \frac{\left( 1 - \bar{\alpha}_t - \left(1-\alpha_t\right)\right)\mathbf{y}_t}{\sqrt{\alpha_t}\left( 1 - \bar{\alpha}_t\right)} + \frac{\beta_t \sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_t}\mathbf{y}_0 \\
&amp; =  \frac{\alpha_t \left( 1 - \bar{\alpha}_{t-1}\right)\mathbf{y}_t}{\sqrt{\alpha_t}\left( 1 - \bar{\alpha}_t\right)} + \frac{\beta_t \sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_t}\mathbf{y}_0 \\
&amp; =  \frac{\sqrt{\alpha_t} \left( 1 - \bar{\alpha}_{t-1}\right)}{1 - \bar{\alpha}_t} \mathbf{y}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t }{1 - \bar{\alpha}_t}\mathbf{y}_0
\end{align*}\]</span></p>
<p>So we can alternatively use a model <span class="math inline">\(\mathbf{s}_\theta (\mathbf{y}_t, t)\)</span> to predict the score <span class="math inline">\(\nabla_{\mathbf{y}_t} \log q(\mathbf{y}_t)\)</span> and define the objective:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}_{t, q(\mathbf{y}_0), q\left(\mathbf{y}_t \mid \mathbf{y}_0 \right)} \biggl[\frac{\lambda(t)}{2}
\| \mathbf{s}_\theta\left(\mathbf{y}_t, t\right) - \nabla_{\mathbf{y}_t} \log q \left(\mathbf{y}_t \mid \mathbf{y}_0 \right)
||^2_2
\biggr]
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is again a weighting function. This makes the relationship between DDPMs and NCSNs obvious, since the denoising score-matching objective is also used in NCSNs.</p>
</section>
<section id="a-common-framework" class="level2">
<h2 class="anchored" data-anchor-id="a-common-framework">A common framework</h2>
<p><span class="citation" data-cites="song2021scorebased">Song et al. (<a href="#ref-song2021scorebased" role="doc-biblioref">2021</a>)</span> explain that the diffusion processes of DDPMs and NCSNs can be described as an Ito SDE</p>
<p><span class="math display">\[\begin{align*}
\mathrm{d} \mathbf{y} = \mathbf{f}(\mathbf{y}, t) \ \mathrm{d}t + g(t) \ \mathrm{d}\mathbf{w}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\mathbf{f}\)</span> is called drift coefficient, <span class="math inline">\(\mathbf{g}\)</span> is called diffusion coefficient and <span class="math inline">\(\mathbf{w}\)</span> is a Wiener process. The process starts at a data sample <span class="math inline">\(\mathbf{y}(0) \sim q(\mathbf{y}(0))\)</span> and continuously corrupts it. The DDPM and NCSC are two special discrete-time cases of this process. In case of the DDPM objective, the corresponding SDE is</p>
<p><span class="math display">\[\begin{align*}
\mathrm{d} \mathbf{y} = -\frac{1}{2} \beta(t) \mathbf{y} \ \mathrm{d}t + \sqrt{\beta(t)} \ \mathrm{d} \mathbf{w}
\end{align*}\]</span></p>
<p>while for the NCSN we have</p>
<p><span class="math display">\[\begin{align*}
\mathrm{d} \mathbf{y} = \sqrt{\frac{\mathrm{d}\sigma^2(t)}{\mathrm{d}t}} \ \mathrm{d}\mathbf{w}
\end{align*}\]</span></p>
<p>These two SDEs specify the forward diffusion processes. We train a score-based SDE by optimising the score matching loss</p>
<p><span class="math display">\[\begin{align*}
\frac{1}{2}\mathbb{E}_{t, q(\mathbf{y}(0)), q(\mathbf{y}(t) \mid \mathbf{y}(0))}  \biggl[\lambda(t)
\| \mathbf{s}_\theta\left(\mathbf{y}(t), t \right) - \nabla_{\mathbf{y}(t)} \log q_{0t} \left(\mathbf{y}(t) \mid \mathbf{y}(0)\right)
||^2_2
\biggr]
\end{align*}\]</span></p>
<p>where <span class="math inline">\(q_{st}(\mathbf{y}(t) \mid \mathbf{y}(s))\)</span> is a transition kernel that generates <span class="math inline">\(\mathbf{y}(t)\)</span> from <span class="math inline">\(\mathbf{y}_s\)</span>.</p>
<p>After having trained this objective, we are interested in using the reverse process again to geerate samples. Remarkably, <span class="citation" data-cites="anderson1982reverse">Anderson (<a href="#ref-anderson1982reverse" role="doc-biblioref">1982</a>)</span> state that the reverse of diffusion process is also a diffusion process that runs backward in time. Specifically:</p>
<p><span class="math display">\[\begin{align*}
\mathrm{d} \mathbf{y} = \left[ \mathbf{f}(\mathbf{y}, t)  - g(t)^2 \nabla_\mathbf{y} \log q_t(\mathbf{y}) \right] \mathrm{d}t + g(t) \mathrm{d}\bar{\mathbf{w} }
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\bar{\mathbf{w}}\)</span> is a Wiener process that runs reverse in time and <span class="math inline">\(t\)</span> is a negative timestep. So, in order to train a score-based SDE need to define the forward process. Two options of which are the DDPM and the NCSN, but in general we are free to define any SDE that is a Ito process.</p>
<p>A particularly useful insight fom the paper is that for every diffusion process there exists a corresponding deterministic diffusion process that has the same marginal densitities <span class="math inline">\(q_t\)</span> as the SDE. This deterministic is an ODE</p>
<p><span class="math display">\[\begin{align*}
\mathrm{d} \mathbf{y} = \left[ \mathbf{f}(\mathbf{y}, t)  - \frac{1}{2} g(t)^2 \nabla_\mathbf{y} \log q_t(\mathbf{y}) \right] \mathrm{d}t
\end{align*}\]</span></p>
<p>which can be solved using any publically available ODE solver.</p>
</section>
</section>
<section id="use-cases" class="level1">
<h1>Use cases</h1>
<p>Let’s implement these three models and compare them. We use the nice Gaussians data set again as in the last case study on diffusion models. The nine Gaussians data set is shown below.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> jnp.array([<span class="op">-</span><span class="fl">2.0</span>, <span class="fl">0.0</span>, <span class="fl">2.0</span>])</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> jnp.array(jnp.meshgrid(means, means)).T.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>covs <span class="op">=</span> jnp.tile((<span class="dv">1</span> <span class="op">/</span> <span class="dv">16</span> <span class="op">*</span> jnp.eye(<span class="dv">2</span>)), [K, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> distrax.Uniform().sample(seed<span class="op">=</span>random.PRNGKey(<span class="dv">23</span>), sample_shape<span class="op">=</span>(K,))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> probs <span class="op">/</span> jnp.<span class="bu">sum</span>(probs)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> distrax.MixtureSameFamily(</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    distrax.Categorical(probs<span class="op">=</span>probs),</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    distrax.MultivariateNormalFullCovariance(means, covs),</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> d.sample(seed<span class="op">=</span>random.PRNGKey(<span class="dv">12345</span>), sample_shape<span class="op">=</span>(n,))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(np.asarray(y), columns<span class="op">=</span>[<span class="st">"x"</span>, <span class="st">"y"</span>])</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.kdeplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">"x"</span>, y<span class="op">=</span><span class="st">"y"</span>, fill<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">"mako_r"</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$y_0$"</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"$y_1$"</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="score_based_sdes_files/figure-html/cell-3-output-1.png" width="520" height="288"></p>
</div>
</div>
<p>Before we start with implementing a model, let’s define an optimizer. We will be able to use the same optimizer for each model.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>prng_seq <span class="op">=</span> hk.PRNGSequence(<span class="dv">1</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>num_batches <span class="op">=</span> y.shape[<span class="dv">0</span>] <span class="op">//</span> batch_size</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> jnp.arange(y.shape[<span class="dv">0</span>])</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> optim(params, opt_state, n_iter<span class="op">=</span><span class="dv">2000</span>):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">@jax.jit</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(params, state, rng, <span class="op">**</span>batch):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> loss_fn(params):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> model.<span class="bu">apply</span>(params, rng, method<span class="op">=</span><span class="st">"loss"</span>, is_training<span class="op">=</span><span class="va">True</span>, <span class="op">**</span>batch)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> jnp.mean(loss)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        loss, grads <span class="op">=</span> jax.value_and_grad(loss_fn)(params)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        updates, new_state <span class="op">=</span> optimizer.update(grads, state, params)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        new_params <span class="op">=</span> optax.apply_updates(params, updates)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss, new_params, new_state</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> n_iter</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_iter):</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            ret_idx <span class="op">=</span> lax.dynamic_slice_in_dim(idxs, j <span class="op">*</span> batch_size, batch_size)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            batch <span class="op">=</span> lax.index_take(y, (ret_idx,), axes<span class="op">=</span>(<span class="dv">0</span>,))</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>            batch_loss, params, opt_state <span class="op">=</span> step(</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>                params, opt_state, <span class="bu">next</span>(prng_seq), y<span class="op">=</span>batch</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> batch_loss</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        losses[i] <span class="op">=</span> loss</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params, losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="score-model" class="level2">
<h2 class="anchored" data-anchor-id="score-model">Score model</h2>
<p>We start by implementing a model, i.e., the neural network that estimates the score, noise, etc. We can use the same model for NCSNs, DDPMs, and score-based SDEs. We can also use the same embedding function for the time points and noise levels used in the NCSNs and DDPMs. As a model, we use a simple MLP with <code>gelu</code> activations and a normalisation layer at the end.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_embedding(inputs, embedding_dim, max_positions<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(inputs.shape) <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    half_dim <span class="op">=</span> embedding_dim <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> jnp.log(max_positions) <span class="op">/</span> (half_dim <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> jnp.exp(jnp.arange(half_dim, dtype<span class="op">=</span>jnp.float32) <span class="op">*</span> <span class="op">-</span>emb)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> inputs[:, <span class="va">None</span>] <span class="op">*</span> emb[<span class="va">None</span>, :]</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> jnp.concatenate([jnp.sin(emb), jnp.cos(emb)], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> emb</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ScoreModel(hk.Module):</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    output_dim <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    hidden_dims <span class="op">=</span> [<span class="dv">256</span>, <span class="dv">256</span>]</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    embedding_dim <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, z, t, is_training):</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        dropout_rate <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> is_training <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        t_embedding <span class="op">=</span> jax.nn.gelu(</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>            hk.Linear(<span class="va">self</span>.embedding_dim)(get_embedding(t, <span class="va">self</span>.embedding_dim))</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> hk.Linear(<span class="va">self</span>.embedding_dim)(z)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        h <span class="op">+=</span> t_embedding</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> dim <span class="kw">in</span> <span class="va">self</span>.hidden_dims:</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> hk.Linear(dim)(h)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> jax.nn.gelu(h)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> hk.LayerNorm(axis<span class="op">=-</span><span class="dv">1</span>, create_scale<span class="op">=</span><span class="va">True</span>, create_offset<span class="op">=</span><span class="va">True</span>)(h)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> hk.dropout(hk.next_rng_key(), dropout_rate, h)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> hk.Linear(<span class="va">self</span>.output_dim)(h)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> h</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="ncsn" class="level2">
<h2 class="anchored" data-anchor-id="ncsn">NCSN</h2>
<p>We start with the NCSN. The implementation is fairly straight-forward and exactly as described above. It consists of a loss function, and a function to sample new data using annealed Langevin dynamics. We provide a score model and a noise schedule to the constructor and that’s all that is needed.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NCSN(hk.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, score_model, sigmata):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.score_model <span class="op">=</span> score_model</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigmata <span class="op">=</span> sigmata</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, method<span class="op">=</span><span class="st">"loss"</span>, <span class="op">**</span>kwargs):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">getattr</span>(<span class="va">self</span>, method)(<span class="op">**</span>kwargs)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, y, is_training<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _log_pdf(y, mu, scale):</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            scale <span class="op">=</span> jnp.full(mu.shape, scale)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> distrax.MultivariateNormalDiag(mu, scale).log_prob(y)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        stds <span class="op">=</span> random.choice(</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>            hk.next_rng_key(), </span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>            a<span class="op">=</span><span class="va">self</span>.sigmata, </span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>            shape<span class="op">=</span>(y.shape[<span class="dv">0</span>],)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        ).reshape(</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>            <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        noise <span class="op">=</span> random.normal(hk.next_rng_key(), y.shape)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        perturbed_y <span class="op">=</span> y <span class="op">+</span> noise <span class="op">*</span> stds[:, <span class="va">None</span>]</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> <span class="va">self</span>.score_model(perturbed_y, stds, is_training)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> jax.vmap(grad(_log_pdf))(perturbed_y, y, stds[:, <span class="va">None</span>])  </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> stds<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> jnp.<span class="bu">sum</span>(jnp.square(score <span class="op">-</span> target), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, sample_shape<span class="op">=</span>(<span class="dv">1</span>,), n<span class="op">=</span><span class="dv">100</span>, eps<span class="op">=</span><span class="fl">2e-5</span>):</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _fn(i, z):</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            alpha <span class="op">=</span> eps <span class="op">*</span> <span class="va">self</span>.sigmata[i] <span class="op">/</span> <span class="va">self</span>.sigmata[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>            sigma <span class="op">=</span> jnp.repeat(<span class="va">self</span>.sigmata[i], z.shape[<span class="dv">0</span>])</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> jnp.arange(n):</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>                noise <span class="op">=</span> random.normal(hk.next_rng_key(), z.shape)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>                z <span class="op">=</span> (</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>                    z</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>                    <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> alpha <span class="op">*</span> <span class="va">self</span>.score_model(z, sigma, is_training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>                    <span class="op">+</span> jnp.sqrt(alpha) <span class="op">*</span> noise</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> z</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>        z_T <span class="op">=</span> random.normal(hk.next_rng_key(), sample_shape <span class="op">+</span> (<span class="dv">2</span>,))</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        z0 <span class="op">=</span> hk.fori_loop(<span class="dv">0</span>, <span class="bu">len</span>(<span class="va">self</span>.sigmata), _fn, z_T)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using Haiku, we need to <code>transform</code> the intialisation of a NCSN object. As a noise schedule, we use a geometric sequence from 1 to 0.01, similarly as to the paper.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _ncsn(<span class="op">**</span>kwargs):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    score_model <span class="op">=</span> ScoreModel()</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> NCSN(score_model, jnp.geomspace(<span class="fl">1.0</span>, <span class="fl">0.01</span>, <span class="dv">100</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model(<span class="op">**</span>kwargs)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> hk.transform(_ncsn)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> model.init(random.PRNGKey(<span class="dv">0</span>), y<span class="op">=</span>y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s optimize this and produce some samples.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optax.adamw(<span class="fl">0.001</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>opt_state <span class="op">=</span> optimizer.init(params)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>params, losses <span class="op">=</span> optim(params, opt_state)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> jnp.asarray(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Having trained the model, let’s visualize the trace of the loss and draw some samples. The samples should look similarly to the original data set, the 9 Gaussians.</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> model.<span class="bu">apply</span>(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    params, random.PRNGKey(<span class="dv">0</span>), method<span class="op">=</span><span class="st">"sample"</span>, sample_shape<span class="op">=</span>(<span class="dv">1000</span>,), n<span class="op">=</span><span class="dv">100</span>, eps<span class="op">=</span><span class="fl">2e-5</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot(losses, samples):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    sns.lineplot(</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        data<span class="op">=</span>pd.DataFrame({<span class="st">"y"</span>: np.asarray(losses), <span class="st">"x"</span>: <span class="bu">range</span>(<span class="bu">len</span>(losses))}),</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="st">"y"</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x"</span>,</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">"black"</span>,</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        ax<span class="op">=</span>ax1</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    sns.kdeplot(</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        data<span class="op">=</span>pd.DataFrame(np.asarray(samples), columns<span class="op">=</span>[<span class="st">"x"</span>, <span class="st">"y"</span>]),</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x"</span>,</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="st">"y"</span>,</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        fill<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        cmap<span class="op">=</span><span class="st">"mako_r"</span>,</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        ax<span class="op">=</span>ax2</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    ax1.<span class="bu">set</span>(title<span class="op">=</span><span class="st">"Loss profile"</span>, xlabel<span class="op">=</span><span class="st">""</span>, ylabel<span class="op">=</span><span class="st">"Loss"</span>, xticks<span class="op">=</span>[], xticklabels<span class="op">=</span>[], yticks<span class="op">=</span>[], yticklabels<span class="op">=</span>[])</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    ax2.<span class="bu">set</span>(title<span class="op">=</span><span class="st">"Generated samples"</span>, xlabel<span class="op">=</span><span class="st">"$y_0$"</span>, ylabel<span class="op">=</span><span class="st">"$y_1$"</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>plot(losses, samples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="score_based_sdes_files/figure-html/cell-9-output-1.png" width="792" height="382"></p>
</div>
</div>
</section>
<section id="ddpm" class="level2">
<h2 class="anchored" data-anchor-id="ddpm">DDPM</h2>
<p>The DDPM implementation looks almost identical. Instead of a noise schedule, we supply a vector of <span class="math inline">\(\beta\)</span>s. In comparison to the NCSN, we don’t need to use Langevin dynamics to sample new data, but can run the Markov chain in reverse. Here, we choose to use the parameterization from the paper which predicts the noise <span class="math inline">\(\boldsymbol \epsilon\)</span> that is used to generate a latent variable <span class="math inline">\(\mathbf{y}_t\)</span>, but it is equivalent to predicing the mean of the forward process posterior.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DDPM(hk.Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, score_model, betas):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.score_model <span class="op">=</span> score_model</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_diffusions <span class="op">=</span> <span class="bu">len</span>(betas)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.betas <span class="op">=</span> betas</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alphas <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> <span class="va">self</span>.betas</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alphas_bar <span class="op">=</span> jnp.cumprod(<span class="va">self</span>.alphas)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sqrt_alphas_bar <span class="op">=</span> jnp.sqrt(<span class="va">self</span>.alphas_bar)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sqrt_1m_alphas_bar <span class="op">=</span>jnp.sqrt(<span class="fl">1.0</span> <span class="op">-</span> <span class="va">self</span>.alphas_bar)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, method<span class="op">=</span><span class="st">"loss"</span>, <span class="op">**</span>kwargs):</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">getattr</span>(<span class="va">self</span>, method)(<span class="op">**</span>kwargs)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, y, is_training<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> random.choice(</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>            key<span class="op">=</span>hk.next_rng_key(),</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>            a<span class="op">=</span>jnp.arange(<span class="dv">0</span>, <span class="va">self</span>.n_diffusions),</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>            shape<span class="op">=</span>(y.shape[<span class="dv">0</span>],),</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        ).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        noise <span class="op">=</span> random.normal(hk.next_rng_key(), y.shape)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        perturbed_y <span class="op">=</span> (</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.sqrt_alphas_bar[t] <span class="op">*</span> y <span class="op">+</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.sqrt_1m_alphas_bar[t] <span class="op">*</span> noise</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        eps <span class="op">=</span> <span class="va">self</span>.score_model(</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>            perturbed_y,</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>            t.reshape(<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>            is_training,</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> jnp.<span class="bu">sum</span>(jnp.square(noise <span class="op">-</span> eps), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, sample_shape<span class="op">=</span>(<span class="dv">1</span>,)):</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _fn(i, x):</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>            t <span class="op">=</span> <span class="va">self</span>.n_diffusions <span class="op">-</span> i</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> random.normal(hk.next_rng_key(), x.shape)</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>            sc <span class="op">=</span> <span class="va">self</span>.score_model(</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>                x,</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>                jnp.full(x.shape[<span class="dv">0</span>], t),</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>                <span class="va">False</span>,</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>            xn <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alphas[t]) <span class="op">/</span> <span class="va">self</span>.sqrt_1m_alphas_bar[t] <span class="op">*</span> sc</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>            xn <span class="op">=</span> x <span class="op">-</span> xn</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>            xn <span class="op">=</span> xn <span class="op">/</span> jnp.sqrt(<span class="va">self</span>.alphas[t])</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> xn <span class="op">+</span> <span class="va">self</span>.betas[t] <span class="op">*</span> z</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>        z_T <span class="op">=</span> random.normal(hk.next_rng_key(), sample_shape <span class="op">+</span> (<span class="dv">2</span>,))</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>        z0 <span class="op">=</span> hk.fori_loop(<span class="dv">0</span>, <span class="va">self</span>.n_diffusions, _fn, z_T)</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Just as in the original publication, we define a <span class="math inline">\(\beta\)</span>-schedule a linear sequence.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _ddpm(<span class="op">**</span>kwargs):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    score_model <span class="op">=</span> ScoreModel()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> DDPM(score_model, jnp.linspace(<span class="fl">10e-4</span>, <span class="fl">0.02</span>, <span class="dv">100</span>))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model(<span class="op">**</span>kwargs)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> hk.transform(_ddpm)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> model.init(random.PRNGKey(<span class="dv">0</span>), y<span class="op">=</span>y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s again optimize this and generate some new data.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optax.adamw(<span class="fl">0.001</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>opt_state <span class="op">=</span> optimizer.init(params)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>params, losses <span class="op">=</span> optim(params, opt_state)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> jnp.asarray(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> model.<span class="bu">apply</span>(params, random.PRNGKey(<span class="dv">0</span>), method<span class="op">=</span><span class="st">"sample"</span>, sample_shape<span class="op">=</span>(<span class="dv">1000</span>,))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plot(losses, samples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="score_based_sdes_files/figure-html/cell-13-output-1.png" width="792" height="382"></p>
</div>
</div>
</section>
<section id="score-based-sde" class="level2">
<h2 class="anchored" data-anchor-id="score-based-sde">Score-based SDE</h2>
<p>Finally, we implement a continuous-time diffusion model using stochastic differential equations. Again, the base implementation is fairly similar to the models before. The only thing we have to adapt in comparison to the NCSN is that we now provide a function that computes the mean and standard deviation for the transition kernel <span class="math inline">\(p_{st}(\mathbf{y}(t) \mid \mathbf{y}(s))\)</span> and one function that computes the drift and diffusion coefficients of the forward and reverse SDEs (which are identical in both directions).</p>
<p>Here, for some variate we will choose the <em>sub variance-preserving SDE</em> from the original publication:</p>
<p><span class="math display">\[\begin{align*}
\mathrm{d} \mathbf{y} &amp; = \mathbf{f}(\mathbf{y}, t) \mathrm{d}t + g(t)\mathrm{d}\mathbf{w} \\
&amp; = - \frac{1}{2} \beta(t)\mathbf{y}\mathrm{d}t + \sqrt{\beta{t}(1-\exp(-2 \smallint_0^t \beta(s) \mathrm{d}s))} \mathrm{d}\mathbf{w}
\end{align*}\]</span></p>
<p>The respective transition kernel is given by</p>
<p><span class="math display">\[\begin{align*}
p_{0t}(\mathbf{y}(t) \mid \mathbf{y}(0)) = \mathcal{N}\left( \mathbf{y}_0 \exp \left( -\tfrac{1}{2}\smallint_0^t\beta(s) \mathrm{d}s \right), \left[1 - \exp \left( -\tfrac{1}{2}\smallint_0^t \beta(s) \right) \mathrm{d}s \right]^2\mathbf{I}
\right)
\end{align*}\]</span></p>
<p>The implementations of the SDE and a function that computes the mean and standard deviation of the transition kernel are shown below:</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> beta_fn(t, beta_max, beta_min):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta_min <span class="op">+</span> t <span class="op">*</span> (beta_max <span class="op">-</span> beta_min)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> integral(t, beta_max, beta_min):</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta_min<span class="op">*</span>t <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> (beta_max <span class="op">-</span> beta_min) <span class="op">*</span> t<span class="op">**</span><span class="dv">2</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sde(x, t, beta_max, beta_min):</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    beta_t <span class="op">=</span> beta_fn(t, beta_max, beta_min)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    intr <span class="op">=</span> integral(t, beta_max, beta_min)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    drift <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> x <span class="op">*</span> beta_t</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    diffusion <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> jnp.exp(<span class="op">-</span><span class="fl">2.0</span> <span class="op">*</span> intr)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    diffusion <span class="op">=</span> jnp.sqrt(beta_t <span class="op">*</span> diffusion)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> drift, diffusion</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> p_mean_scale(x, t, beta_max, beta_min):</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    intr <span class="op">=</span> integral(t, beta_max, beta_min)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> x <span class="op">*</span> jnp.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> intr)[:, <span class="va">None</span>]</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    std <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> jnp.exp(<span class="op">-</span>intr)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean, std</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>p_mean_scale_fn <span class="op">=</span> partial(p_mean_scale, beta_min<span class="op">=</span><span class="fl">0.1</span>, beta_max<span class="op">=</span><span class="fl">10.0</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>sde_fn <span class="op">=</span> partial(sde, beta_min<span class="op">=</span><span class="fl">0.1</span>, beta_max<span class="op">=</span><span class="fl">10.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The constructor of the score-based SDE takes the score model, the function that computes the parameters of the transition kernel and the SDE itself.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ScoreSDE(hk.Module):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, score_model, p_mean_scale, sde):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.score_model <span class="op">=</span> score_model</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.T <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.p_mean_scale <span class="op">=</span> p_mean_scale</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sde <span class="op">=</span> sde</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, method<span class="op">=</span><span class="st">"loss"</span>, <span class="op">**</span>kwargs):</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">getattr</span>(<span class="va">self</span>, method)(<span class="op">**</span>kwargs)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, y, is_training<span class="op">=</span><span class="va">True</span>, eps<span class="op">=</span><span class="fl">1e-8</span>):</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _log_pdf(y, mu, scale):</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>            scale <span class="op">=</span> jnp.full(mu.shape, scale)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> distrax.MultivariateNormalDiag(mu, scale).log_prob(y)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> jax.random.uniform(</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>            hk.next_rng_key(), (y.shape[<span class="dv">0</span>],), minval<span class="op">=</span>eps, maxval<span class="op">=</span><span class="va">self</span>.T</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        ).reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> jax.random.normal(hk.next_rng_key(), y.shape)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        mean, scale <span class="op">=</span> <span class="va">self</span>.p_mean_scale(y, t)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        perturbed_y <span class="op">=</span> mean <span class="op">+</span> z <span class="op">*</span> scale[:, <span class="va">None</span>]</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> <span class="va">self</span>.score_model(perturbed_y, t, is_training)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> jnp.<span class="bu">sum</span>((score <span class="op">*</span> scale[:, <span class="va">None</span>] <span class="op">+</span> z) <span class="op">**</span> <span class="dv">2</span>, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, sample_shape<span class="op">=</span>(<span class="dv">1</span>,), is_training<span class="op">=</span><span class="va">False</span>, eps<span class="op">=</span><span class="fl">1e-8</span>):</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> jax.random.normal(hk.next_rng_key(), sample_shape <span class="op">+</span> (<span class="dv">2</span>,))</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        _, scale <span class="op">=</span> <span class="va">self</span>.p_mean_scale(y, jnp.atleast_1d(<span class="va">self</span>.T))</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        x_init <span class="op">=</span> z <span class="op">*</span> scale</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> ode_func(t, x):</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>            drift, diffusion <span class="op">=</span> <span class="va">self</span>.sde(x, jnp.atleast_1d(t))</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>            t <span class="op">=</span> np.full((x.shape[<span class="dv">0</span>],), t)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>            score <span class="op">=</span> <span class="va">self</span>.score_model(x, t, is_training)</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>            ret <span class="op">=</span> drift <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> (diffusion<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> score</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> ret.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> integrate.solve_ivp(</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>            ode_func,</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>            (<span class="va">self</span>.T, eps),</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>            np.asarray(x_init).reshape(<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>            rtol<span class="op">=</span><span class="fl">1e-5</span>,</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>            atol<span class="op">=</span><span class="fl">1e-5</span>,</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>            method<span class="op">=</span><span class="st">"RK45"</span>,</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> res.y[:, <span class="op">-</span><span class="dv">1</span>].reshape(x_init.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s test the score-based SDE. We use the same procedure as above and again train with AdamW.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _score_sde(<span class="op">**</span>kwargs):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    score_model <span class="op">=</span> ScoreModel()</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ScoreSDE(score_model, p_mean_scale_fn, sde_fn)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model(<span class="op">**</span>kwargs)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> hk.transform(_score_sde)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> model.init(random.PRNGKey(<span class="dv">0</span>), y<span class="op">=</span>y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optax.adamw(<span class="fl">0.001</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>opt_state <span class="op">=</span> optimizer.init(params)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>params, losses <span class="op">=</span> optim(params, opt_state)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> jnp.asarray(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s again draw some samples and look at the loss profile.</p>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> model.<span class="bu">apply</span>(params, random.PRNGKey(<span class="dv">0</span>), method<span class="op">=</span><span class="st">"sample"</span>, sample_shape<span class="op">=</span>(<span class="dv">1000</span>,))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plot(losses, samples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="score_based_sdes_files/figure-html/cell-18-output-1.png" width="792" height="382"></p>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>This case study implemented three recent developments in generative diffusions. We first implemented the NCSN and DDPM objectives, and then the framework by <span class="citation" data-cites="song2021scorebased">Song et al. (<a href="#ref-song2021scorebased" role="doc-biblioref">2021</a>)</span> which turned out to be a continuous time generalisation of both models. In terms of sample quality, for the nine Gaussians data set the results are somewhat similar.</p>
</section>
<section id="session-info" class="level1">
<h1>Session info</h1>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> session_info</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>session_info.show(html<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-----
arviz               0.12.0
distrax             0.1.2
haiku               0.0.9
jax                 0.4.5
jaxlib              0.4.4
matplotlib          3.6.2
numpy               1.24.2
optax               0.1.3
palettes            NA
pandas              1.5.1
scipy               1.10.1
seaborn             0.11.2
session_info        1.0.0
-----
IPython             8.4.0
jupyter_client      7.3.4
jupyter_core        4.10.0
jupyterlab          3.3.4
notebook            6.4.12
-----
Python 3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:27:43) [Clang 11.1.0 ]
macOS-13.0.1-arm64-i386-64bit
-----
Session information updated at 2023-03-04 13:09</code></pre>
</div>
</div>
</section>
<section id="license" class="level1">
<h1>License</h1>
<p><a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img align="left" alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png"></a> <br><br></p>
<p>The case study is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.</p>
<section id="references" class="level2 unnumbered">


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-anderson1982reverse" class="csl-entry" role="doc-biblioentry">
Anderson, Brian DO. 1982. <span>“Reverse-Time Diffusion Equation Models.”</span> <em>Stochastic Processes and Their Applications</em> 12 (3): 313–26.
</div>
<div id="ref-ho2020denoising" class="csl-entry" role="doc-biblioentry">
Ho, Jonathan, Ajay Jain, and Pieter Abbeel. 2020. <span>“Denoising Diffusion Probabilistic Models.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 6840–51.
</div>
<div id="ref-hyvarinen2005estimation" class="csl-entry" role="doc-biblioentry">
Hyvärinen, Aapo, and Peter Dayan. 2005. <span>“Estimation of Non-Normalized Statistical Models by Score Matching.”</span> <em>Journal of Machine Learning Research</em> 6 (4).
</div>
<div id="ref-sohl2015deep" class="csl-entry" role="doc-biblioentry">
Sohl-Dickstein, Jascha, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. <span>“Deep Unsupervised Learning Using Nonequilibrium Thermodynamics.”</span> In <em>International Conference on Machine Learning</em>, 2256–65. PMLR.
</div>
<div id="ref-song2019generative" class="csl-entry" role="doc-biblioentry">
Song, Yang, and Stefano Ermon. 2019. <span>“Generative Modeling by Estimating Gradients of the Data Distribution.”</span> <em>Advances in Neural Information Processing Systems</em> 32.
</div>
<div id="ref-song2021scorebased" class="csl-entry" role="doc-biblioentry">
Song, Yang, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021. <span>“Score-Based Generative Modeling Through Stochastic Differential Equations.”</span> In <em>International Conference on Learning Representations</em>.
</div>
<div id="ref-vincent2011connection" class="csl-entry" role="doc-biblioentry">
Vincent, Pascal. 2011. <span>“A Connection Between Score Matching and Denoising Autoencoders.”</span> <em>Neural Computation</em> 23 (7): 1661–74.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>