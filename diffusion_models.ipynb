{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Diffusion models\n",
        "author: Simon Dirmeier\n",
        "date: 'June, 2022'\n",
        "bibliography: ./references/references.bib\n",
        "link-citations: true\n",
        "format:\n",
        "  html:\n",
        "    theme:\n",
        "      - lumen\n",
        "      - css/custom.scss\n",
        "toc: true\n",
        "toc-depth: 2\n",
        "toc-float:\n",
        "  collapsed: 'no'\n",
        "  smooth_scroll: 'yes'\n",
        "  number_sections: 'no'\n",
        "toc-location: right\n",
        "toc-title: ''\n",
        "linkcolor: '#1F407A'\n",
        "date-format: 'MMMM, YYYY'\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Diffusion probabilistic models (DPMs), or generative diffusion processes, have attracted significant attention for generative modelling in the last couple of months. Similiarly to normalizing flows, DPMs model data iteratively via a set of transformations. The main idea of DPMs is to first add noise to a data set and then learn a reverse Markovian process that denoises the disrupted data and thus allows generating data from white noise. In this case study, we'll reimplement the the vanilla model introduced in @sohl2015deep. To implement the models, we'll use Jax, Haiku, Distrax and Optax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jnp, lax, nn, random\n",
        "import optax\n",
        "import haiku as hk\n",
        "import distrax\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import arviz as az\n",
        "import palettes\n",
        "\n",
        "sns.set(rc={\"figure.figsize\": (6, 3)}) \n",
        "sns.set_style(\"ticks\", {'font.family': 'serif', 'font.serif': 'Merriweather'})\n",
        "palettes.set_theme()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diffusion models\n",
        "\n",
        "We briefly discuss diffusion probabilistic models as introduced in @sohl2015deep. For details, please refer to the original paper or @ho2020denoising. Diffusion models are latent variable models of the form\n",
        "\n",
        "$$\n",
        "p_\\theta \\left( \\mathbf{y} \\right)  = \\int p_\\theta \\left( \\mathbf{y}, \\mathbf{z}_{1:T} \\right) d\\mathbf{z}_{1:T}\n",
        "$$\n",
        "\n",
        "The above marginal is obtained by integrating over latent variables $\\mathbf{z}_{1:T}$ which have the same dimensionality as the data $\\mathbf{y}$. The joint distribution $p_\\theta \\left( \\mathbf{y}, \\mathbf{z}_{1:T} \\right)$ is defined via learned Markovian transitions\n",
        "\n",
        "$$\n",
        "p_\\theta \\left( \\mathbf{y}, \\mathbf{z}_{1:T} \\right) =  p_\\theta(\\mathbf{x} \\mid \\mathbf{z}_1) \\prod_{t=2}^T  p_\\theta(\\mathbf{z}_{t - 1} \\mid \\mathbf{z}_t)  \\; p(\\mathbf{z}_T)\n",
        "$$\n",
        "\n",
        "where $p_\\theta$ is parameterized via a neural network. In comparison to other latent variable models, however, diffusion models start my assuming a forward process that iteratively corrupts a data set $\\mathbf{x} \\sim q(\\mathbf{y})$ via diffusions\n",
        "\n",
        "$$\n",
        "q\\left(\\mathbf{z}_{1:T}, \\mathbf{y}\\right) = q(\\mathbf{y}) q(\\mathbf{z}_{1} \\mid \\mathbf{y}) \\prod_{t=2}^T q(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t - 1}) \n",
        "$$\n",
        "\n",
        "that have a fixed schedule\n",
        "\n",
        "$$\n",
        "q(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t - 1}) = \\mathcal{N}\\left( \\sqrt{1 - \\beta_i} \\mathbf{z}_{t - 1}, \\beta_i \\mathbf{I}  \\right)\n",
        "$$\n",
        "\n",
        "Hence, diffusion probablistic models assume a fixed approximate posterior and learn the generative model $p_\\theta$.\n",
        "\n",
        "# Implementation\n",
        "\n",
        "Using the equations above, we can implement a DPM ourselves without much coding. With JAX and Haiku, a DPM could be implemented like that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class DPM(hk.Module):\n",
        "    def __init__(self, beta_schedule, reverse_process):\n",
        "        super().__init__()\n",
        "        self._reverse_process = reverse_process\n",
        "        self.n_diffusions = len(beta_schedule)\n",
        "        self.beta_schedule = beta_schedule\n",
        "\n",
        "    def __call__(self, method=\"reverse_loc_and_log_scale\", **kwargs):\n",
        "        return getattr(self, method)(**kwargs)\n",
        "\n",
        "    def _diffuse(self, z, beta):\n",
        "        e = distrax.Normal(jnp.zeros_like(z), 1.0).sample(seed=hk.next_rng_key())\n",
        "        z = jnp.sqrt(1.0 - beta) * z + jnp.sqrt(beta) * e\n",
        "        return z\n",
        "\n",
        "    def reverse_loc_and_log_scale(self, y):\n",
        "        # forward diffusion\n",
        "        zs = [y] + [None] * self.n_diffusions\n",
        "        for i, beta in enumerate(self.beta_schedule):\n",
        "            zs[i + 1] = self._diffuse(zs[i], beta)\n",
        "\n",
        "        # reverse diffusion\n",
        "        locs, log_scales = [None] * self.n_diffusions, [None] * self.n_diffusions\n",
        "        for i in np.arange(self.n_diffusions - 1, -1, -1):\n",
        "            loc, log_scale = jnp.split(self._reverse_process(zs[i + 1]), 2, axis=-1)\n",
        "            locs[i] = loc\n",
        "            log_scales[i] = log_scale\n",
        "\n",
        "        return zs, locs, log_scales\n",
        "\n",
        "    def reverse_diffusion(self, z):\n",
        "        for _ in np.arange(self.n_diffusions):\n",
        "            loc, log_scale = jnp.split(self._reverse_process(z), 2, axis=-1)\n",
        "            e = distrax.Normal(jnp.zeros_like(z), 1.0).sample(seed=hk.next_rng_key())\n",
        "            z = loc + jnp.exp(log_scale) * e\n",
        "        return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function `reverse_loc_and_log_scale` first computes the forward process to sample all latent variables, and then, starting from the $\\mathbf{z}_T$, computes the reverse process, or rather the locations and scales of each Gaussian transition, which are parameterized by a neural network.\n",
        "\n",
        "Using Haiku, we construct and initialize a DPM like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_diffusions = 5\n",
        "beta_schedule = jnp.linspace(10e-4, 0.02, n_diffusions)\n",
        "\n",
        "def _dm(method, **kwargs):\n",
        "    reverse_process =  hk.Sequential([\n",
        "        hk.Linear(256), jax.nn.leaky_relu,\n",
        "        hk.Linear(256), jax.nn.leaky_relu,\n",
        "        hk.Linear(256), jax.nn.leaky_relu,\n",
        "        hk.Linear(2 * 2),\n",
        "    ])\n",
        "    return DPM(beta_schedule, reverse_process)(method, **kwargs)\n",
        "\n",
        "diffusion = hk.transform(_dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the model above, we are only using 5 diffusions. In practice, we should set this number higher to ensure that the distribution of last latent variable $\\mathbf{z}_T$ is approximately standard normal. However, as we will see later, optimizing an objective with $T = 1000$ is extremely inefficient and for demonstration we limit ourselves to only 5 diffusions.\n",
        "\n",
        "# Data\n",
        "\n",
        "Let's test the model on a synthetic data set. We sample data from the frequently found \"nine Gaussians\" distribution. The data set consists of nine fairly well separated Gaussian distributions which is a fairly difficult data set to learn the density of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "K = 9\n",
        "\n",
        "means = jnp.array([-2.0, 0.0, 2.0])\n",
        "means = jnp.array(jnp.meshgrid(means, means)).T.reshape(-1, 2)\n",
        "covs = jnp.tile((1 / 16 * jnp.eye(2)), [K, 1, 1])\n",
        "\n",
        "probs = distrax.Uniform().sample(seed=random.PRNGKey(23), sample_shape=(K,))\n",
        "probs = probs / jnp.sum(probs)\n",
        "\n",
        "d = distrax.MixtureSameFamily(\n",
        "    distrax.Categorical(probs=probs),\n",
        "    distrax.MultivariateNormalFullCovariance(means, covs)\n",
        ")\n",
        "\n",
        "n = 10000\n",
        "y = d.sample(seed=random.PRNGKey(2), sample_shape=(n,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sampled data looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.DataFrame(np.asarray(y), columns=[\"x\", \"y\"])\n",
        "ax = sns.kdeplot(\n",
        "    data=df, x=\"x\", y=\"y\", fill=True, cmap=\"mako_r\"\n",
        ")\n",
        "ax.set_xlabel(\"$y_0$\")\n",
        "ax.set_ylabel(\"$y_1$\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we train the model, we define some helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def timer(func):\n",
        "    from timeit import default_timer\n",
        "    def f(*args, **kwargs):\n",
        "        start = default_timer()\n",
        "        res = func(*args, **kwargs)\n",
        "        stop = default_timer()\n",
        "        print(f\"Elapsed time: {stop - start}\")\n",
        "        return res\n",
        "    return f\n",
        "\n",
        "def _normal_from_beta(z, beta):\n",
        "    return distrax.Independent(\n",
        "        distrax.Normal(\n",
        "            jnp.sqrt(1.0 - beta) * z,\n",
        "            jnp.sqrt(beta)\n",
        "        )\n",
        "    )\n",
        "\n",
        "def _normal(loc, log_scale):\n",
        "    return distrax.Independent(\n",
        "        distrax.Normal(\n",
        "            loc,\n",
        "            jnp.exp(log_scale)\n",
        "        )\n",
        "    )\n",
        "\n",
        "def _std_normal(like):\n",
        "    return distrax.Independent(\n",
        "        distrax.Normal(\n",
        "            jnp.zeros_like(like), jnp.ones_like(like)\n",
        "        )\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ELBO\n",
        "\n",
        "Training of diffusion models is performed by optimizing the usual evidence lower bound (ELBO):\n",
        "\n",
        "\\begin{align*}\n",
        "\\log p_\\theta(\\mathbf{y}) \\ge \\text{ELBO}(q) = \\mathbb{E}_{q(\\mathbf{z}_{1:T})} \n",
        "\\Bigl[\n",
        "& \\log p_\\theta(\\mathbf{y} \\mid \\mathbf{z}_{1}) \\\\\n",
        " &+ \\sum_{i=1}^{T - 1} \\log p_\\theta(\\mathbf{z}_i \\mid \\mathbf{z}_{i + 1}) - \\sum_{i=2}^{T} \\log q(\\mathbf{z}_i \\mid \\mathbf{z}_{i - 1})  \\\\\n",
        " & +  \\log p_\\theta(\\mathbf{z}_T) - \\log q(\\mathbf{z}_1 \\mid \\mathbf{y})\n",
        "\\Bigr]\n",
        "\\end{align*}\n",
        "\n",
        "We first initialize our diffusion model to get a `pytree` of parameters which we need for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "params = diffusion.init(\n",
        "    random.PRNGKey(2), \n",
        "    y=y, \n",
        "    method=\"reverse_loc_and_log_scale\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We optimize the ELBO using Optax. A single gradient step using Optax and the ELBO defined above looks, for instance, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "adam = optax.adamw(0.001)\n",
        "opt_state = adam.init(params)\n",
        "\n",
        "@jax.jit\n",
        "def step(params, opt_state, y, rng):\n",
        "    def loss_fn(params):\n",
        "        zs, locs, log_scales = diffusion.apply(\n",
        "            params, rng=rng, y=y, method=\"reverse_loc_and_log_scale\"\n",
        "        )\n",
        "\n",
        "        # log likelihood p(y | z_1)\n",
        "        log_pxz = _normal(locs[0], log_scales[0]).log_prob(y)\n",
        "\n",
        "        kl = 0.0\n",
        "        # note that: zs[0] == y\n",
        "        for i in np.arange(1, len(zs)):\n",
        "            # q(z_i | z_{i - 1}) where zs[0] = y\n",
        "            lp_q = _normal_from_beta(zs[i - 1], beta_schedule[i - 1]).log_prob(zs[i])\n",
        "\n",
        "            # p(z_i | z_{i + 1})\n",
        "            if i != n_diffusions:\n",
        "                lp_p = _normal(locs[i], log_scales[i]).log_prob(zs[i])\n",
        "            # p(z_T)\n",
        "            else:\n",
        "                lp_p = _std_normal(zs[i]).log_prob(zs[i])\n",
        "\n",
        "            kli = lp_q - lp_p\n",
        "            kl += kli\n",
        "\n",
        "        loss = -jnp.sum(log_pxz - kl)\n",
        "        return loss\n",
        "\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
        "    updates, new_opt_state = adam.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return loss, new_params, new_opt_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use batch sizes of 128 and run the optimizer for 2000 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prng_seq = hk.PRNGSequence(42)\n",
        "batch_size = 128\n",
        "num_batches = y.shape[0] // batch_size\n",
        "idxs = jnp.arange(y.shape[0])\n",
        "\n",
        "@timer\n",
        "def optim(params, opt_state, n_iter = 2000):    \n",
        "    losses = [0] * n_iter\n",
        "    for i in range(n_iter):\n",
        "        loss = 0.0\n",
        "        for j in range(batch_size):\n",
        "            ret_idx = lax.dynamic_slice_in_dim(idxs, j * batch_size, batch_size)\n",
        "            batch = lax.index_take(y, (ret_idx,), axes=(0,))\n",
        "            batch_loss, params, opt_state = step(params, opt_state, batch, next(prng_seq))\n",
        "            loss += batch_loss\n",
        "        losses[i] = loss\n",
        "    return params, losses\n",
        "\n",
        "params, losses = optim(params, opt_state)\n",
        "losses = jnp.asarray(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This took quite some time even though we only used five diffusion steps. It also demonstrates why this objective is not very efficient to compute and prohibits a larger number of diffusions. Before we derive a more efficient objective, let's have a look at some plots and if training actually worked.\n",
        "\n",
        "Let's have a look if the ELBO converged:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ax = sns.lineplot(\n",
        "    data=pd.DataFrame({\"y\": np.asarray(losses), \"x\": range(len(losses))}),\n",
        "    y=\"y\", x=\"x\",\n",
        "    color='black'\n",
        ")\n",
        "ax.set(\n",
        "    xlabel=\"\", ylabel=\"-ELBO\",\n",
        "    xticks=[], xticklabels=[],\n",
        "    yticks=[], yticklabels=[]\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Having trained the model, we can sample from the diffusion model like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prior = distrax.Normal(jnp.zeros(2), jnp.ones(2))\n",
        "z =  prior.sample(\n",
        "    seed=random.PRNGKey(33),\n",
        "    sample_shape=(5000,)\n",
        ")\n",
        "y_hat = diffusion.apply(\n",
        "    params, rng=random.PRNGKey(1), \n",
        "    z=z, method=\"reverse_diffusion\"\n",
        ")\n",
        "\n",
        "ax = sns.kdeplot(\n",
        "    data=pd.DataFrame(np.asarray(y_hat), columns=[\"x\", \"y\"]),\n",
        "    x=\"x\", y=\"y\", fill=True, cmap=\"mako_r\"\n",
        ")\n",
        "ax.set(xlabel=\"$y_0$\", ylabel=\"$y_1$\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But for the sake of demonstration, this worked nicely! Visually, the estimated density is somewhat close to the original data set. To improve it, we could, for instance, increase the number of diffusions or use a more suitable network architecture. \n",
        "\n",
        "# A better objective\n",
        "\n",
        "Given this simple neural network architecture and low sample size, training the objective took inacceptably much time. We can, however, use the following insight to define an objective that is easier to train. Since the forward transitions are all Gaussians, we can analytically integrate out intermediate steps, yielding:\n",
        "$$\n",
        "q \\left(  \\mathbf{z}_t \\mid \\mathbf{y} \\right)  = \n",
        "\\mathcal{N} \\left( \\sqrt{\\bar{\\alpha}_t}\\mathbf{y}, \\left( 1 - \\bar{\\alpha}_t \\right) \\mathbf{I} \\right)\n",
        "$$\n",
        "\n",
        "Using Bayes rule, we can in addition derive the posterior of this process using:\n",
        "\n",
        "\\begin{align*}\n",
        "q \\left(  \\mathbf{z}_{t - 1} \\mid  \\mathbf{z}_{t}, \\mathbf{y} \\right)  &  =  \n",
        "\\frac{q \\left(  \\mathbf{z}_{t} \\mid  \\mathbf{z}_{t-1} , \\mathbf{y} \\right) q\\left(  \\mathbf{z}_{t-1} \\mid  \\mathbf{y} \\right)  }{q \\left(  \\mathbf{z}_{t} \\mid  \\mathbf{y} \\right) } \\\\\n",
        "&  = \\mathcal{N} \\left( \\tilde{\\boldsymbol \\mu}_t\\left( \\mathbf{z}_{t}, \\mathbf{y}\\right) , \\tilde{\\beta}_t \\mathbf{I} \\right)\n",
        "\\end{align*}\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\tilde{\\boldsymbol \\mu}_t\\left( \\mathbf{z}_{t}, \\mathbf{y}\\right) = \n",
        "\\frac{\\sqrt{\\bar{\\alpha}_{t - 1}} \\beta_t }{1 - \\bar{\\alpha}_{t}} \\mathbf{y} +\n",
        "\\frac{\\sqrt{\\alpha} \\left( 1 - \\bar{\\alpha}_{t - 1} \\right) }{1 - \\bar{\\alpha}_{t}} \\mathbf{z}_t \n",
        "$$\n",
        "\n",
        "and \n",
        "$$\n",
        "\\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t - 1}}{1 - \\bar{\\alpha}_{t}} {\\beta}_t\n",
        "$$\n",
        "\n",
        "Plugin these derivations into the ELBO gives us the following:\n",
        "$$\n",
        "\\mathbb{E}_{q, t} \\biggl[ \n",
        "\\log p_\\theta \\left(\\mathbf{y}, \\mathbf{z}_1 \\right) -\n",
        "\\mathbb{KL}\\Bigl[ q(\\mathbf{z}_{t - 1} \\mid \\mathbf{z}_{t}, \\mathbf{y}), p_\\theta(\\mathbf{z}_{t - 1} \\mid \\mathbf{z}_t)  \\Bigr] -\n",
        "\\mathbb{KL}\\Bigl[ q(\\mathbf{z}_T \\mid \\mathbf{y}), p_\\theta(\\mathbf{z}_T)  \\Bigr]\n",
        "\\biggr]\n",
        "$$\n",
        "\n",
        "Instead of sampling all T $\\mathbf{z}_i$s, we can instead only optimze the first and the last part and one summand of the sum over the $t$s\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{q, t} \\biggl[ \n",
        "\\log p_\\theta \\left(\\mathbf{y}, \\mathbf{z}_1 \\right) -\n",
        "\\mathbb{KL}\\Bigl[ q(\\mathbf{z}_T \\mid \\mathbf{y}), p_\\theta(\\mathbf{z}_T)  \\Bigr] -\n",
        "\\mathbb{KL}\\Bigl\n",
        "[ q(\\mathbf{z}_{t - 1} \\mid \\mathbf{z}_{t}, \\mathbf{y}), p_\\theta(\\mathbf{z}_{t - 1} \\mid \\mathbf{z}_t)  \\Bigr] \n",
        "\\biggr]\n",
        "$$\n",
        "\n",
        "In addition, if we reparameterize $\\mathbf{z}_t$, we get\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{q(\\mathbf{y}), t \\sim \\mathcal{U}, p(\\boldsymbol \\epsilon_t), p(\\boldsymbol \\epsilon_T)} \\biggl[ \n",
        "\\log p_\\theta \\left(\\mathbf{y}, \\mathbf{z}_1 \\right) -\n",
        "\\mathbb{KL} \\Bigl[ q(\\mathbf{z}_{t - 1} \\mid \\mathbf{z}_{t}, \\mathbf{y}), p_\\theta(\\mathbf{z}_{t - 1} \\mid \\mathbf{z}_t)  \\Bigr] -\n",
        "\\mathbb{KL} \\Bigl[ q(\\mathbf{z}_T \\mid \\mathbf{y}), p_\\theta(\\mathbf{z}_T)  \\Bigr] \n",
        "\\biggr]\n",
        "$$\n",
        "\n",
        "In order to compute $q \\left(  \\mathbf{z}_t \\mid \\mathbf{y} \\right)$ and $q \\left(  \\mathbf{z}_{t - 1} \\mid  \\mathbf{z}_{t}, \\mathbf{y} \\right)$ we update our DPM class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class DPM(hk.Module):\n",
        "    def __init__(self, beta_schedule, reverse_process):\n",
        "        super().__init__()\n",
        "        self._reverse_process = reverse_process\n",
        "        self.n_diffusions = len(beta_schedule)\n",
        "        self.beta_schedule = beta_schedule\n",
        "\n",
        "    def __call__(self, method=\"reverse_process\", **kwargs):\n",
        "        return getattr(self, method)(**kwargs)\n",
        "\n",
        "    def reverse_diffusion(self, z):\n",
        "        for _ in np.arange(self.n_diffusions):\n",
        "            loc, log_scale = jnp.split(self._reverse_process(z), 2, axis=-1)\n",
        "            e = distrax.Normal(jnp.zeros_like(z), 1.0).sample(seed=hk.next_rng_key())\n",
        "            z = loc + jnp.exp(log_scale) * e\n",
        "        return z\n",
        "\n",
        "    def _alpha_bar(self):\n",
        "        alphas = 1.0 - self.beta_schedule\n",
        "        alphas_bar = jnp.cumprod(alphas)\n",
        "        return alphas_bar\n",
        "\n",
        "    def _beta_tilde(self, t):\n",
        "        alphas_bar = self._alpha_bar()\n",
        "        return (1.0 - alphas_bar[t - 1]) / (1.0 - alphas_bar[t]) * self.beta_schedule[t]\n",
        "\n",
        "    def reverse_process(self, z):\n",
        "        loc, log_scale = jnp.split(self._reverse_process(z), 2, axis=-1)\n",
        "        return distrax.Independent(\n",
        "            distrax.Normal(loc, jnp.exp(log_scale)), 1\n",
        "        )\n",
        "\n",
        "    def forward_process(self, y, t):\n",
        "        alphas_bar = self._alpha_bar()\n",
        "        return distrax.Independent(\n",
        "            distrax.Normal(\n",
        "                jnp.sqrt(alphas_bar[t]) * y,\n",
        "                jnp.repeat(jnp.sqrt(1.0 - alphas_bar[t]), y.shape[-1])\n",
        "            ), 1\n",
        "        )\n",
        "\n",
        "    def sample_forward_process(self, y, t, epsilon=None):\n",
        "        alphas_bar = self._alpha_bar()\n",
        "        if epsilon is None:\n",
        "            z = distrax.MultivariateNormalDiag(\n",
        "                jnp.sqrt(alphas_bar[t]) * y,\n",
        "                jnp.repeat(1.0 - alphas_bar[t], y.shape[-1])\n",
        "            ).sample(seed=hk.next_rng_key())\n",
        "        else:\n",
        "            z = jnp.sqrt(alphas_bar[t]) * y + jnp.sqrt(1.0 - alphas_bar[t]) * epsilon\n",
        "        return z\n",
        "\n",
        "    def posterior_forward_process(self, y, zt, t):\n",
        "        alphas = 1.0 - self.beta_schedule\n",
        "        alphas_bar = self._alpha_bar()\n",
        "        beta_tilde = self._beta_tilde(t)\n",
        "\n",
        "        lhs = (jnp.sqrt(alphas_bar[t - 1]) * self.beta_schedule[t])\n",
        "        lhs = lhs / (1.0 - alphas_bar[t]) * y\n",
        "\n",
        "        rhs = jnp.sqrt(alphas[t]) * (1.0 - alphas_bar[t - 1])\n",
        "        rhs = rhs / (1.0 - alphas_bar[t]) * zt\n",
        "\n",
        "        return distrax.Independent(\n",
        "            distrax.Normal(\n",
        "                lhs + rhs,\n",
        "                jnp.repeat(jnp.sqrt(beta_tilde), y.shape[-1])\n",
        "            ), 1\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since our new ELBO consists only of three terms, we can increase the number of diffusions. Here, we set it to 100:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_diffusions = 100\n",
        "beta_schedule = jnp.linspace(10e-4, 0.02, n_diffusions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the ELBO is similar to the above. We first define the model again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _dm(method, **kwargs):\n",
        "    reverse_process = hk.Sequential([\n",
        "        hk.Linear(256), jax.nn.leaky_relu,\n",
        "        hk.Linear(256), jax.nn.leaky_relu,        \n",
        "        hk.Linear(256), jax.nn.leaky_relu,        \n",
        "        hk.Linear(2 * 2),\n",
        "    ])\n",
        "    return DPM(beta_schedule, reverse_process)(method, **kwargs)\n",
        "\n",
        "diffusion = hk.transform(_dm)\n",
        "params = diffusion.init(\n",
        "    random.PRNGKey(23),\n",
        "    z=y,\n",
        "    method=\"reverse_process\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define our updated objective:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "adam = optax.adamw(0.001)\n",
        "opt_state = adam.init(params)\n",
        "np.random.seed(2)\n",
        "\n",
        "@jax.jit\n",
        "def step(params, opt_state, y, rng):\n",
        "    def loss_fn(params):\n",
        "        t = np.random.choice(np.arange(1, n_diffusions))\n",
        "\n",
        "        ## compute the last term: KL between priors\n",
        "        q_z_T = diffusion.apply(\n",
        "            params, rng=rng, y=y, t=n_diffusions - 1, method=\"forward_process\"\n",
        "        )\n",
        "        p_z_T = distrax.Independent(\n",
        "            distrax.Normal(jnp.zeros_like(y), 1), 1\n",
        "        )\n",
        "        # KL q(z_T | y) || p(z_T)\n",
        "        kl_T = q_z_T.kl_divergence(p_z_T)\n",
        "\n",
        "        ## compute the middle term: KL between two adjacent t's\n",
        "        sample_rng, new_rng = random.split(rng)\n",
        "        e_t = distrax.Normal(jnp.zeros_like(y), 1.0).sample(seed=sample_rng)        \n",
        "        z_t = diffusion.apply(\n",
        "            params, rng=rng, y=y, t=t, epsilon=e_t,\n",
        "            method=\"sample_forward_process\"\n",
        "        )\n",
        "        q_z_tm1 = diffusion.apply(\n",
        "            params, rng=rng, y=y, zt=z_t, t=t,\n",
        "            method=\"posterior_forward_process\"\n",
        "        )\n",
        "        p_z_tm1 = diffusion.apply(\n",
        "            params, rng=rng, z=z_t, method=\"reverse_process\"\n",
        "        )\n",
        "        # KL q(z{t - 1} | Z_t, Y) || p(z_{t - 1} | z_t)\n",
        "        kl = q_z_tm1.kl_divergence(p_z_tm1)\n",
        "\n",
        "        ## compute the first term: log likeihood\n",
        "        sample_rng, new_rng = random.split(new_rng)\n",
        "        e_1 = distrax.Normal(jnp.zeros_like(y), 1.0).sample(seed=sample_rng)\n",
        "        z_1 = diffusion.apply(\n",
        "            params, rng=rng, y=y, t=0, epsilon=e_1, method=\"sample_forward_process\"\n",
        "        )\n",
        "        p_z_1 = diffusion.apply(\n",
        "            params, rng=rng, z=z_1, method=\"reverse_process\"\n",
        "        )        \n",
        "        # log likelihood P(Y | Z_1)\n",
        "        log_pxz = p_z_1.log_prob(y)\n",
        "        \n",
        "        loss = -jnp.sum(log_pxz - kl - kl_T)\n",
        "        return loss\n",
        "\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
        "    updates, new_opt_state = adam.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return loss, new_params, new_opt_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we train the model. The training procedure is exactly the same as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prng_seq = hk.PRNGSequence(1)\n",
        "batch_size = 128\n",
        "num_batches = y.shape[0] // batch_size\n",
        "idxs = jnp.arange(y.shape[0])\n",
        "\n",
        "@timer\n",
        "def optim(params, opt_state, n_iter = 2000):    \n",
        "    losses = [0] * n_iter\n",
        "    for i in range(n_iter):\n",
        "        loss = 0.0\n",
        "        for j in range(batch_size):\n",
        "            ret_idx = lax.dynamic_slice_in_dim(idxs, j * batch_size, batch_size)\n",
        "            batch = lax.index_take(y, (ret_idx,), axes=(0,))\n",
        "            batch_loss, params, opt_state = step(params, opt_state, batch, next(prng_seq))\n",
        "            loss += batch_loss\n",
        "        losses[i] = loss\n",
        "    return params, losses\n",
        "\n",
        "params, losses = optim(params, opt_state)\n",
        "losses = jnp.asarray(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training this objective is significantly faster than the original one, despite increasing the number of diffusions. Let's have a look at the ELBO again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ax = sns.lineplot(\n",
        "  d  ata=pd.DataFrame({\"y\": np.asarray(losses), \"x\": range(len(losses))}),\n",
        "    y=\"y\", x=\"x\",\n",
        "    color='black'\n",
        ")\n",
        "ax.set(\n",
        "    xlabel=\"\", ylabel=\"-ELBO\",\n",
        "    xticks=[], xticklabels=[],\n",
        "    yticks=[], yticklabels=[]\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the end, let's also sample some data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prior = distrax.Normal(jnp.zeros(2), jnp.ones(2))\n",
        "z = prior.sample(\n",
        "    seed=random.PRNGKey(33),\n",
        "    sample_shape=(5000,)\n",
        ")\n",
        "y_hat = diffusion.apply(\n",
        "    params, rng=random.PRNGKey(1), \n",
        "    z=z, method=\"reverse_diffusion\"\n",
        ")\n",
        "\n",
        "ax = sns.kdeplot(\n",
        "  data=pd.DataFrame(np.asarray(y_hat), columns=[\"x\", \"y\"]),\n",
        "  x=\"x\", y=\"y\", fill=True, cmap=\"mako_r\"\n",
        ")\n",
        "ax.set(xlabel=\"$y_0$\", ylabel=\"$y_1$\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As before, the density of the data was estimated fairly well given the simple model architecture.\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "DPMs are an exciting new class of models for generative modelling and density estimation. Even though the model was originally published in 2015 already, recent interest was (afaict) mainly sparked by the follow-up papers by @ho2020denoising and @dhariwal2021diffusion which demonstrated that DPMs are SOTA generative models, e.g., in image generation. The next [case-study](https://dirmeier.github.io/etudes/denoising_diffusion_models.html)) will demonstrate the improvements made by @ho2020denoising.\n",
        "\n",
        "# Session info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import session_info\n",
        "session_info.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# License\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a>\n",
        "\n",
        "The notebook is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.\n",
        "\n",
        "\n",
        "# References"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}